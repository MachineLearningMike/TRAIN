{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]\n",
      "matplotlib 3.7.0\n",
      "numpy 1.23.5\n",
      "pandas 1.5.3\n",
      "sklearn 1.2.1\n",
      "tensorflow 2.12.0\n",
      "keras.api._v2.keras 2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_params_test import *\n",
    "from train_utility import *\n",
    "from trans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = None\n",
    "if len(gpus) > 1: \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    Learning_Rate = Learning_Rate * len(gpus) * 3 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_file(path):\n",
    "    with open(path, \"+tw\") as f:\n",
    "        f.write(\"# Sorry, the content is removed.\")\n",
    "        f.write(\"\\n# Please ask Mike for the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/mnt/data/Trading/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m folder \u001b[39min\u001b[39;00m folders:\n\u001b[0;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n\u001b[1;32m----> 4\u001b[0m         os\u001b[39m.\u001b[39;49mmkdir(folder)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/mnt/data/Trading/'"
     ]
    }
   ],
   "source": [
    "folders = [dir_data, dir_datasets, dir_candles, dir_Checkpoint, dir_CSVLogs]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "Candles = np.swapaxes(Candles, 0, 1)\n",
    "print(\"Candles: {}\".format(Candles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[:, market, :], 1, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(Candles[:, market, :], 3, 30, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'Candles'. ====================\n",
    "# Candles.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "CandleMarks = Candles[:, :, 9] # keep it for later use\n",
    "Candles = np.delete(Candles, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, CandleMarks\n",
    "assert (~np.isfinite(Candles)).any() == False\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "markets = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert Candles.shape[1] == len(markets)\n",
    "print(Candles.shape, len(markets), markets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts, interval_s, timestamps_abs = get_timestamps(CandleFile, Candles)\n",
    "print(start_ts, interval_s, timestamps_abs.shape, timestamps_abs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define Data ====================\n",
    "\n",
    "Data = Candles[:, :, :]   # (time:, all markets, 20 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Find marketrank ==================\n",
    "\n",
    "check = np.array([ np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100 for m in range(len(markets)) ])\n",
    "permute = np.argsort(check)\n",
    "marketrank = [ (markets[m], 100 - round(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) for m in permute ]\n",
    "# marketrank = [ markets[m] for m in permute ]\n",
    "\n",
    "batch = 10\n",
    "for i in range(0, len(markets), batch):\n",
    "    print(marketrank[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "x_indices, y_indices, chosen_markets, chosen_fields = \\\n",
    "get_formation_params(\n",
    "    enFields, markets, marketrank,\n",
    "    min_true_candle_percent_x, chosen_fields_x_names, min_true_candle_percent_y, chosen_fields_y_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_usdt = len(\"USDT\")\n",
    "chosen_market_names = [markets[market][:-len_usdt] for market in chosen_markets]\n",
    "batch = 10\n",
    "for i in range(0, len(chosen_market_names), batch):\n",
    "    print(chosen_market_names[i: i+batch])\n",
    "print(len(chosen_market_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_eFree_with_plot'. Ugly coupling.\n",
    "event_free_data_loss = 3 * ( alpha * SmallSigma + LargeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - event_free_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        sSigma = SmallSigma\n",
    "        if enFields[field] == 'BaseVolume': sSigma = SmallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_eFree_with_plot(markets[market], enFields[field], Data[:, market, field], sSigma, \n",
    "                            LargeSigma, Data.shape[0] - event_free_data_loss, noPlot=eFreeNoPlot, noLog=eFreeNoLog)\n",
    "        Data[event_free_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[event_free_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = get_time_features(timestamps_abs)\n",
    "size_time = Time.shape[1]\n",
    "\n",
    "Time = Time[event_free_data_loss: ]\n",
    "assert Data.shape[0] == Time.shape[0]\n",
    "print(Candles.shape, Time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard = None\n",
    "\n",
    "if Standardization:\n",
    "    Data, Standard = standardize(Data, chosen_markets, chosen_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        ax.plot(Data[:, market, field], label = \"{} @ {}\".format(enFields[field], markets[market][:-4])) # -4: 'USDT'\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444., 555.', 2, 4, 2, 2, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11.', 2, 4, 2, 2, True, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 2, 2, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444.', 2, 4, 1, 4, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 1, 4, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defind test data\n",
    "\n",
    "n_times = 1000; n_markets = 2; n_fields = 2\n",
    "data = [ [ [ time * n_markets * n_fields + market * n_fields + field for field in range(n_fields) ] for market in range(n_markets) ] for time in range(n_times)]\n",
    "data = np.array(data, dtype=float)\n",
    "times_test = np.array( range(data.shape[0]) ) + 100000\n",
    "print(data.shape, times_test.shape)   # time, market, field\n",
    "print(data[:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, times_test.shape)   # time, market, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_test = 2\n",
    "ny_test = 2\n",
    "ns_test = 10\n",
    "batchSize = 2\n",
    "\n",
    "sample_anchors = range(0, data.shape[0] - nx_test - ny_test, ns_test)\n",
    "print(data.shape[0], len(sample_anchors), sample_anchors)\n",
    "\n",
    "x_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "y_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "print(data[0:2][:, x_indices[0]][:, :, x_indices[1]])\n",
    "print(data[2:4][:, y_indices[0]][:, :, y_indices[1]])\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "size_time = 1\n",
    "print(size_x, size_y, size_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_plus = CandleFile+'_o'\n",
    "name_prefix = os.path.join(dir_datasets, name_plus)\n",
    "\n",
    "reuse_files = False\n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus))\n",
    "    filenames = divide_to_multiple_csv_files(data, True, True, times_test, sample_anchors, name_prefix, nx_test, x_indices, ny_test, y_indices, header=None, n_parts=10)\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(filenames, shuffle=None) # no way to prevent shuffle.\n",
    "print(filename_dataset.cardinality().numpy())\n",
    "for element in filename_dataset:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(filenames[0])\n",
    "for line in ds.take(20):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    print(element)\n",
    "    break\n",
    "\n",
    "# should print: (None, nx_test, size_x + size_time), (None, ny_test * size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_reader_to_dataset(filenames, nx_test, size_x, ny_test, size_y, True, True, size_time,\n",
    "                             n_parse_threads=5, batch_size=batchSize, shuffle_buffer_size=1000, n_readers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check elements: NaN, -inf, +inf\n",
    "\n",
    "assert (~np.isfinite(Candles)).any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLatest = 500\n",
    "P, maP, logP, log_maP, event, eventFree = Get_eFree(Candles[:, 0, 0], 1, 30, nLatest)\n",
    "assert maP.shape[0] == nLatest; assert logP.shape[0] == nLatest; assert log_maP.shape[0] == nLatest; assert event.shape[0] == nLatest; assert eventFree.shape[0] == nLatest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename),\n",
    "    cycle_length=n_readers, num_parallel_calls=tf.data.AUTOTUNE) # no way to prevent shuffle?\n",
    "\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_anchores_t, sample_anchores_v = get_sample_anchores(Data, Nx, Ny, Ns)\n",
    "\n",
    "print(len(sample_anchores_t), len(sample_anchores_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_train, Dataset_valid, dx, dy = get_datasets(\n",
    "    Reuse_files,\n",
    "    CandleFile, dir_datasets, Data, Time_into_X, Time_into_Y, Time, \n",
    "    sample_anchores_t, sample_anchores_v,\n",
    "    Nx, x_indices, Ny, y_indices, nFiles_t, nFiles_v, n_readers, size_time,\n",
    "    BatchSize, shuffle_batch, Transformer, nPrefetch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "if mirrored_strategy is None:\n",
    "    model = build_model(\n",
    "        dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "        HuberThreshold, CancleLossWeight, TrendLossWeight\n",
    "    )\n",
    "else:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_model(\n",
    "            dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "            HuberThreshold, CancleLossWeight, TrendLossWeight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(\n",
    "    checkpoint_filepath, Checkpoint_Monitor, \n",
    "    csvLogger_filepath, \n",
    "    EarlyStopping_Min_Monitor, EarlyStopping_Patience\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"Failed to load a checkpoint\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    Dataset_train, # x and y_true\n",
    "    validation_data=Dataset_valid, \n",
    "    epochs=20, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'trans_mTA', 'val_trans_mTA', 'trans_1_loss', 'val_trans_1_loss')\n",
    "plot_csv_log_history(csvLogger_filepath, columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
