{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_params_test import *\n",
    "from train_utility import *\n",
    "from trans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_file(path):\n",
    "    with open(path, \"+tw\") as f:\n",
    "        f.write(\"# Sorry, the content is removed.\")\n",
    "        f.write(\"\\n# Please ask Mike for the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [dir_data, dir_datasets, dir_candles, dir_Checkpoint, dir_CSVLogs]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "Candles = np.swapaxes(Candles, 0, 1)\n",
    "print(\"Candles: {}\".format(Candles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[:, market, :], 1, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(Candles[:, market, :], 3, 30, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'Candles'. ====================\n",
    "# Candles.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "CandleMarks = Candles[:, :, 9] # keep it for later use\n",
    "Candles = np.delete(Candles, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, CandleMarks\n",
    "assert (~np.isfinite(Candles)).any() == False\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "markets = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert Candles.shape[1] == len(markets)\n",
    "print(Candles.shape, len(markets), markets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts, interval_s, timestamps_abs = get_timestamps(CandleFile, Candles)\n",
    "print(start_ts, interval_s, timestamps_abs.shape, timestamps_abs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define Data ====================\n",
    "\n",
    "Data = Candles[:, :, :]   # (time:, all markets, 20 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Find marketrank ==================\n",
    "\n",
    "check = np.array([ np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100 for m in range(len(markets)) ])\n",
    "permute = np.argsort(check)\n",
    "marketrank = [ (markets[m], 100 - round(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) for m in permute ]\n",
    "# marketrank = [ markets[m] for m in permute ]\n",
    "\n",
    "batch = 10\n",
    "for i in range(0, len(markets), batch):\n",
    "    print(marketrank[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "x_indices, y_indices, chosen_markets, chosen_fields = \\\n",
    "get_formation_params(\n",
    "    enFields, markets, marketrank,\n",
    "    min_true_candle_percent_x, chosen_fields_x_names, min_true_candle_percent_y, chosen_fields_y_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_usdt = len(\"USDT\")\n",
    "chosen_market_names = [markets[market][:-len_usdt] for market in chosen_markets]\n",
    "batch = 10\n",
    "for i in range(0, len(chosen_market_names), batch):\n",
    "    print(chosen_market_names[i: i+batch])\n",
    "print(len(chosen_market_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_eFree_with_plot'. Ugly coupling.\n",
    "event_free_data_loss = 3 * ( alpha * SmallSigma + LargeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - event_free_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        sSigma = SmallSigma\n",
    "        if enFields[field] == 'BaseVolume': sSigma = SmallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_eFree_with_plot(markets[market], enFields[field], Data[:, market, field], sSigma, \n",
    "                            LargeSigma, Data.shape[0] - event_free_data_loss, noPlot=eFreeNoPlot, noLog=eFreeNoLog)\n",
    "        Data[event_free_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[event_free_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = get_time_features(timestamps_abs)\n",
    "size_time = Time.shape[1]\n",
    "\n",
    "Time = Time[event_free_data_loss: ]\n",
    "assert Data.shape[0] == Time.shape[0]\n",
    "print(Candles.shape, Time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standard = None\n",
    "\n",
    "if Standardization:\n",
    "    Data, Standard = standardize(Data, chosen_markets, chosen_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        ax.plot(Data[:, market, field], label = \"{} @ {}\".format(enFields[field], markets[market][:-4])) # -4: 'USDT'\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_anchores_t, sample_anchores_v = get_sample_anchores(Data, Nx, Ny, Ns)\n",
    "\n",
    "print(len(sample_anchores_t), len(sample_anchores_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_train, Dataset_valid, dx, dy = get_datasets(\n",
    "    Reuse_files,\n",
    "    CandleFile, dir_datasets, Data, Time_into_X, Time_into_Y, Time, \n",
    "    sample_anchores_t, sample_anchores_v,\n",
    "    Nx, x_indices, Ny, y_indices, nFiles_t, nFiles_v, n_readers, size_time,\n",
    "    BatchSize, shuffle_batch, Transformer, nPrefetch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "if mirrored_strategy is None:\n",
    "    model = build_model(\n",
    "        dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "        HuberThreshold, CancleLossWeight, TrendLossWeight\n",
    "    )\n",
    "else:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_model(\n",
    "            dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "            HuberThreshold, CancleLossWeight, TrendLossWeight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(\n",
    "    checkpoint_filepath, Checkpoint_Monitor, \n",
    "    csvLogger_filepath, \n",
    "    EarlyStopping_Min_Monitor, EarlyStopping_Patience\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"Failed to load a checkpoint\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    Dataset_train, # x and y_true\n",
    "    validation_data=Dataset_valid, \n",
    "    epochs=20, #Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'trans_mTA', 'val_trans_mTA', 'trans_1_loss', 'val_trans_1_loss')\n",
    "plot_csv_log_history(csvLogger_filepath, columns)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
