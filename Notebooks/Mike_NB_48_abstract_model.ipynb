{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mike_NB_tools import *\n",
    "from Transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================  Define training parameters ===========================\n",
    "\n",
    "Nx = 600\n",
    "Ny = 5\n",
    "Ns = 5\n",
    "BatchSize = 64\n",
    "\n",
    "CandleFile = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "SmallSigma = 1\n",
    "LargeSigma = 30\n",
    "eFreeNoLog = True\n",
    "\n",
    "nFiles_t = 70\n",
    "nFiles_v = 30\n",
    "n_readers = 20\n",
    "shuffle_batch = 200\n",
    "nPrefetch = 3\n",
    "\n",
    "dir_datasets = \"/mnt/data/Trading/Datasets\"\n",
    "dir_candles = \"/mnt/data/Trading/Candles\"\n",
    "\n",
    "chosen_markets_x = [\n",
    "                    'DOTUSDT', 'BTCUSDT', 'ETHUSDT', 'LINKUSDT', 'ETCUSDT', 'XLMUSDT', 'STPTUSDT',   # cluster 2, with over 50% true candles, except 'DOTUSDT'\n",
    "                    'EOSUSDT', 'BNTUSDT', # cluster 1, with over 50% true candles.\n",
    "                    'ANKRUSDT', # cluster 0, with over 50% true candles.\n",
    "                    'BNBUSDT', 'AVAXUSDT', 'COMPUSDT', 'BALUSDT', 'XRPUSDT', '1INCHUSDT'  # etc\n",
    "]\n",
    "chosen_fields_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "chosen_markets_y = [\n",
    "                    'BTCUSDT', 'ETHUSDT', 'LINKUSDT', 'ETCUSDT', 'XLMUSDT',   # cluster 2, with over 50% true candles, except 'DOTUSDT'\n",
    "                    'BNBUSDT', 'AVAXUSDT', 'BALUSDT', 'XRPUSDT', '1INCHUSDT'  # etc\n",
    "]\n",
    "chosen_fields_y = ['ClosePrice']\n",
    "\n",
    "Standardization = False\n",
    "Time_into_X = True\n",
    "Time_into_Y = False\n",
    "Transformer = False\n",
    "Reuse_files = False\n",
    "\n",
    "Epochs_Initial = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "Candles = np.swapaxes(Candles, 0, 1)\n",
    "print(\"Candles: {}\".format(Candles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[:, market, :], 1, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(Candles[:, market, :], 3, 30, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'Candles'. ====================\n",
    "# Candles.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "CandleMarks = Candles[:, :, 9] # keep it for later use\n",
    "Candles = np.delete(Candles, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, CandleMarks\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "markets = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert Candles.shape[1] == len(markets)\n",
    "print(Candles.shape, len(markets), markets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Restore timestamps_abs. ====================\n",
    "\n",
    "start = datetime( 2000+int(CandleFile[0:2]), int(CandleFile[3:5]), int(CandleFile[6:8]), int(CandleFile[9:11]), int(CandleFile[12:14]) )\n",
    "start_ts = round(datetime.timestamp(start))\n",
    "interval = CandleFile[ CandleFile.find('-', len(CandleFile) - 4) + 1 : ]\n",
    "interval_s = round(intervalToMilliseconds(interval) / 1000)\n",
    "timestamps_abs = np.array( range(start_ts, start_ts + Candles.shape[0] * interval_s, interval_s), dtype=int)\n",
    "assert timestamps_abs.shape[0] == Candles.shape[0]\n",
    "print(start_ts, interval_s, timestamps_abs.shape, timestamps_abs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444., 555.', 2, 4, 2, 2, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11.', 2, 4, 2, 2, True, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 2, 2, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444., 555.', 2, 4, 2, 2, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11.', 2, 4, 2, 2, True, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 2, 2, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defind test data\n",
    "\n",
    "n_times = 1000; n_markets = 2; n_fields = 2\n",
    "data = [ [ [ time * n_markets * n_fields + market * n_fields + field for field in range(n_fields) ] for market in range(n_markets) ] for time in range(n_times)]\n",
    "data = np.array(data, dtype=float)\n",
    "times_test = np.array( range(data.shape[0]) ) + 100000\n",
    "print(data.shape, times_test.shape)   # time, market, field\n",
    "print(data[:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, times_test.shape)   # time, market, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_test = 2\n",
    "ny_test = 2\n",
    "ns_test = 10\n",
    "batchSize = 2\n",
    "\n",
    "sample_anchors = range(0, data.shape[0] - nx_test - ny_test, ns_test)\n",
    "print(data.shape[0], len(sample_anchors), sample_anchors)\n",
    "\n",
    "x_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "y_indices = ( (0,), (0, 1) )    # (market, field)\n",
    "print(data[0:2][:, x_indices[0]][:, :, x_indices[1]])\n",
    "print(data[2:4][:, y_indices[0]][:, :, y_indices[1]])\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "size_time = 1\n",
    "print(size_x, size_y, size_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_plus = CandleFile+'_o'\n",
    "name_prefix = os.path.join(dir_datasets, name_plus)\n",
    "\n",
    "reuse_files = False\n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus))\n",
    "    filenames = divide_to_multiple_csv_files(data, True, True, times_test, sample_anchors, name_prefix, nx_test, x_indices, ny_test, y_indices, header=None, n_parts=10)\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(filenames, shuffle=None) # no way to prevent shuffle.\n",
    "print(filename_dataset.cardinality().numpy())\n",
    "for element in filename_dataset:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(filenames[0])\n",
    "for line in ds.take(20):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename),\n",
    "    cycle_length=n_readers, num_parallel_calls=tf.data.AUTOTUNE) # no way to prevent shuffle?\n",
    "\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_reader_to_dataset(filenames, nx_test, size_x, ny_test, size_y, True, True, size_time,\n",
    "                             n_parse_threads=5, batch_size=batchSize, shuffle_buffer_size=1000, n_readers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    print(element)\n",
    "    break\n",
    "\n",
    "# should print: (None, nx_test, size_x + size_time), (None, ny_test * size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check elements: NaN, -inf, +inf\n",
    "\n",
    "assert (~np.isfinite(Candles)).any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLatest = 500\n",
    "P, maP, logP, log_maP, event, eventFree = Get_eFree(Candles[:, 0, 0], 1, 30, nLatest)\n",
    "assert maP.shape[0] == nLatest; assert logP.shape[0] == nLatest; assert log_maP.shape[0] == nLatest; assert event.shape[0] == nLatest; assert eventFree.shape[0] == nLatest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'get_eFree_with_plot' ====================\n",
    "\n",
    "def get_eFree_with_plot(market, field, feature, smallSigma, largeSigma, nLatest, noPlot = True, noLog = False):\n",
    "    if noLog:\n",
    "        P, maP, _, _, event, eventFree = Get_eFree_noLog(feature, smallSigma, largeSigma, nLatest)\n",
    "        series = [ [maP, \"maP\", \"g\"], [event, \"event\", \"c\"],  [eventFree, \"e.Free\", \"brown\"] ] #, [P, \"raw feature\", \"r\"] ]\n",
    "        if not noPlot:\n",
    "            PoltNormalized(\"Event-free (brown) {} on {}\".format(field, market), series)\n",
    "        return P, maP, _, _, event, eventFree\n",
    "    else:\n",
    "        P, maP, logP, log_maP, event, eventFree = Get_eFree(feature, smallSigma, largeSigma, nLatest)\n",
    "        series = [ [maP, \"maP\", \"g\"], [logP, \"logP\" ,\"m\"], [log_maP, \"log.maP\", \"b\"], [event, \"event\", \"c\"],  [eventFree, \"e.Free\", \"brown\"] ] #, [P, \"raw feature\", \"r\"] ]\n",
    "        if not noPlot:\n",
    "            PoltNormalized(\"Event-free (brown) {} on {}\".format(field, market), series)\n",
    "        return P, maP, logP, log_maP, event, eventFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [\n",
    "    ['APT', 'SUI', 'DYDX', 'ANKR', 'AUDIO', 'SKL'],\n",
    "    ['EOS', 'AAVE', 'FLOW', 'CRV', 'COMP', 'SLP', 'MBOX', 'BNT', 'SPELL', 'AERGO', 'BAKE'],\n",
    "    ['DOT', 'BTC', 'ETH', 'WBTC', 'LINK', 'ETC', 'XLM', 'TWT', 'SFP', 'STPT', 'STEEM', 'POWR'],\n",
    "]\n",
    "\n",
    "for c in range(len(clusters)):\n",
    "    cluster = clusters[c]\n",
    "    cluster = [ markets.index(m + 'USDT') for m in cluster ]\n",
    "    check = [ (markets[m], 100 - round(np.argmax(Candles[:, m, 0]>0) / Candles.shape[0] * 100)) for m in cluster ]\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define Data ====================\n",
    "\n",
    "Data = Candles[:, :, :]   # (time:, all markets, 20 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.array([ np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100 for m in range(len(markets)) ])\n",
    "permute = np.argsort(check)\n",
    "marketrank = [ (markets[m], 100 - round(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) for m in permute ]\n",
    "# marketrank = [ markets[m] for m in permute ]\n",
    "\n",
    "batch = 10\n",
    "for i in range(0, len(markets), batch):\n",
    "    print(marketrank[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Select markets and fields ====================\n",
    "\n",
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "# dot, 1inch, btc, eth, matic, bnb, ada, sol, ltc, avax, wbtc, link, arb, ape, aave, crv, sui, op, gmx, agix, bal, comp, gmt, joe, stg\n",
    "\n",
    "chosen_markets_x = tuple([ markets.index(elem) for elem in chosen_markets_x ])\n",
    "chosen_markets_x = tuple(list(set(chosen_markets_x)))\n",
    "\n",
    "chosen_fields_x = tuple( [ enFields.index(elem) for elem in chosen_fields_x ] )\n",
    "chosen_fields_x = tuple(list(set(chosen_fields_x)))\n",
    "x_indices = ( chosen_markets_x, chosen_fields_x )\n",
    "print(x_indices)\n",
    "\n",
    "chosen_markets_y = tuple([ markets.index(elem) for elem in chosen_markets_y ])\n",
    "chosen_markets_y = tuple(list(set(chosen_markets_y)))\n",
    "\n",
    "chosen_fields_y = tuple( [ enFields.index(elem) for elem in chosen_fields_y ] )\n",
    "chosen_fields_y = tuple(list(set(chosen_fields_y)))\n",
    "y_indices = ( chosen_markets_y, chosen_fields_y )\n",
    "print(y_indices)\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "print(size_x, size_y)\n",
    "\n",
    "chosen_markets = tuple(list(set(chosen_markets_x + chosen_markets_y)))\n",
    "chosen_fields = tuple(list(set(chosen_fields_x + chosen_fields_y)))\n",
    "print(chosen_markets, chosen_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_eFree_with_plot'. Ugly coupling.\n",
    "event_free_data_loss = 3 * ( alpha * SmallSigma + LargeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - event_free_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        sSigma = SmallSigma\n",
    "        if enFields[field] == 'BaseVolume': sSigma = SmallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_eFree_with_plot(markets[market], enFields[field], Data[:, market, field], sSigma, \n",
    "                            LargeSigma, Data.shape[0] - event_free_data_loss, noPlot=False, noLog=eFreeNoLog)\n",
    "        Data[event_free_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[event_free_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define time features, to augment Data with ====================\n",
    "\n",
    "sigma = np.power(2.0, -0.2)\n",
    "hourly = np.sin( 2 * np.pi / (60*60) * timestamps_abs ) / sigma\n",
    "daily = np.sin( 2 * np.pi / (60*60*24) * timestamps_abs ) / sigma\n",
    "weekly = np.sin( 2 * np.pi / (60*60*24*7) * timestamps_abs ) / sigma\n",
    "yearly = np.sin( 2 * np.pi / (60*60*24*365) * timestamps_abs ) / sigma\n",
    "\n",
    "Time = np.stack([hourly, daily, weekly, yearly], axis=1)\n",
    "size_time = Time.shape[1]\n",
    "\n",
    "Time = Time[event_free_data_loss: ]\n",
    "assert Data.shape[0] == Time.shape[0]\n",
    "print(Candles.shape, Time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Standardize Data on chosen markets and fields ====================\n",
    "\n",
    "Standard = []\n",
    "\n",
    "if Standardization:\n",
    "    for market in chosen_markets:\n",
    "        for field in chosen_fields:\n",
    "            nzPs = np.where( Data[:, market, field] != 0.0 ) [0]\n",
    "            mu = np.average(Data[nzPs, market, field])\n",
    "            sigma = np.std(Data[nzPs, market, field])\n",
    "            standard = (Data[nzPs, market, field] - mu) / (sigma + 1e-15)\n",
    "            Standard.append( (market, field, mu, sigma) )\n",
    "            Data[nzPs, market, field] = standard\n",
    "\n",
    "    Standard = np.array(Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        ax.plot(Data[:, market, field], label = \"{} @ {}\".format(enFields[field], markets[market][:-4])) # -4: 'USDT'\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define input sequence and output sequence ====================\n",
    "\n",
    "sample_anchors = np.array(range(0, Data.shape[0] - Nx - Ny + 1, Ns))\n",
    "print(Data.shape[0], len(sample_anchors), sample_anchors, sample_anchors[-1])\n",
    "print(Data.shape[0], sample_anchors[ -1 ], sample_anchors[ -1 ] + Nx + Ny, sample_anchors[ -1 ] + Ns, sample_anchors[ -1 ] + Ns + Nx + Ny)\n",
    "\n",
    "for _ in range(10):\n",
    "    permute = np.random.permutation(sample_anchors.shape[0])\n",
    "    sample_anchors = sample_anchors[permute]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sample_anchores_t, sample_anchores_v = train_test_split(sample_anchors, test_size=0.30, random_state=42)\n",
    "print(sample_anchores_t.shape, sample_anchores_v.shape)\n",
    "\n",
    "sample_anchores_t = tuple(sample_anchores_t)\n",
    "sample_anchores_v = tuple(sample_anchores_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Create train/valid datasets ====================\n",
    "\n",
    "name_plus_t = CandleFile+'_t'\n",
    "name_plus_v = CandleFile+'_v'\n",
    "name_prefix_t = os.path.join(dir_datasets, name_plus_t)\n",
    "name_prefix_v = os.path.join(dir_datasets, name_plus_v)\n",
    "\n",
    "reuse_files = Reuse_files #------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames_train = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_t, x)]\n",
    "    filenames_valid = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_v, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_t))\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_v))\n",
    "    filenames_train = divide_to_multiple_csv_files(Data, Time_into_X, Time_into_Y, Time, sample_anchores_t, name_prefix_t, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_t)\n",
    "    filenames_valid = divide_to_multiple_csv_files(Data, Time_into_X, Time_into_Y, Time, sample_anchores_v, name_prefix_v, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_v)\n",
    "\n",
    "# sample_anchores are already shuffled. But we need to shuffle datasets again, because it will reshuffle at every epoch.\n",
    "Dataset_train = csv_reader_to_dataset(filenames_train, Nx, size_x, Ny, size_y, Time_into_X, Time_into_Y, size_time,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*shuffle_batch, n_readers=n_readers, transformer=Transformer)\n",
    "Dataset_train = Dataset_train.prefetch(nPrefetch)\n",
    "\n",
    "Dataset_valid = csv_reader_to_dataset(filenames_valid, Nx, size_x, Ny, size_y, Time_into_X, Time_into_Y, size_time,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*shuffle_batch, n_readers=n_readers, transformer=Transformer)\n",
    "Dataset_valid = Dataset_valid.prefetch(nPrefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in Dataset_train:\n",
    "    print(element)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define RegularizedLSTM ====================\n",
    "\n",
    "from functools import partial\n",
    "RegularizedLSTM = partial(keras.layers.LSTM,\n",
    "                          return_sequences=True,\n",
    "                          kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                          recurrent_regularizer=keras.regularizers.l2(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = len(sample_anchores_t) // BatchSize\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001, # default: 0.001 \n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define build_model ====================\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_size, allow_cudnn_kernel=True):\n",
    "\n",
    "    units = max(input_dim, output_size)\n",
    "\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "\n",
    "        inputs = keras.Input( shape = (None, input_dim), name = \"candles\" )\n",
    "        x = keras.layers.BatchNormalization()(inputs)       \n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.LSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        outputs = keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "\n",
    "        model = keras.Model( \n",
    "            inputs = inputs, \n",
    "            outputs = outputs,\n",
    "            name = \"LSTMDense_model\"\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            # loss=keras.losses.MeanAbsoluteError(name='loss'),\n",
    "            loss=MaskedMSE(),\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=0.0001,  # def lr = 0.001\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999, \n",
    "                epsilon=1e-07\n",
    "            ),\n",
    "            metrics=keras.metrics.MSE,\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Build model ====================\n",
    "\n",
    "model = build_model(size_x + (size_time if Time_into_X else 0), Ny * size_y, allow_cudnn_kernel=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Fit model ====================\n",
    "\n",
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)\n",
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
