{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NB21 was enhanced with the following code, over NB20, and showed dramatically reduced losses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/3\n",
    "2023-05-25 19:40:17.275237: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-25 19:40:18.546347: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 194s 277ms/step - loss: 29198790.0000 - mean_squared_error: 29198790.0000 - val_loss: 28408572.0000 - val_mean_squared_error: 28408572.0000\n",
    "Epoch 2/3\n",
    "618/618 [==============================] - 174s 277ms/step - loss: 28434302.0000 - mean_squared_error: 28434302.0000 - val_loss: 28311176.0000 - val_mean_squared_error: 28311176.0000\n",
    "Epoch 3/3\n",
    "618/618 [==============================] - 167s 265ms/step - loss: 28419544.0000 - mean_squared_error: 28419544.0000 - val_loss: 28259174.0000 - val_mean_squared_error: 28259174.0000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB21\n",
    "- Enhanced with 'logarithm + smoothen', which transformes both X and Y data.\n",
    "- Shifted the loss range to a smaller one.\n",
    "- We have to investigate the seemingly successful NB21 and NB31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmize + Smoothen\n",
    "smallSigma = 1\n",
    "largeSigma = 30\n",
    "\n",
    "head_data_loss = 3 * (smallSigma + largeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - head_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_plot_log_feature(markets[market], enFields[field], Data[:, market, field], smallSigma, largeSigma, Data.shape[0] - head_data_loss, NoChart = True)\n",
    "        Data[head_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[head_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/100\n",
    "2023-05-25 22:40:56.585019: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-25 22:40:57.861933: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "Output exceeds the size limit. Open the full output data in a text editor618/618 [==============================] - 195s 286ms/step - loss: 0.0330 - mean_squared_error: 0.0034 - val_loss: 9.0767e-04 - val_mean_squared_error: 0.0037\n",
    "Epoch 2/100\n",
    "618/618 [==============================] - 177s 282ms/step - loss: 7.1860e-04 - mean_squared_error: 0.0034 - val_loss: 7.0616e-04 - val_mean_squared_error: 0.0037\n",
    "Epoch 3/100\n",
    "618/618 [==============================] - 178s 285ms/step - loss: 6.8946e-04 - mean_squared_error: 0.0034 - val_loss: 7.0529e-04 - val_mean_squared_error: 0.0037\n",
    "Epoch 4/100\n",
    "618/618 [==============================] - 177s 283ms/step - loss: 6.8942e-04 - mean_squared_error: 0.0034 - val_loss: 7.0573e-04 - val_mean_squared_error: 0.0037\n",
    "Epoch 5/100\n",
    "618/618 [==============================] - 177s 283ms/step - loss: 6.8910e-04 - mean_squared_error: 0.0034 - val_loss: 7.0552e-04 - val_mean_squared_error: 0.0037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/5\n",
    "2023-05-27 05:33:45.215406: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-27 05:33:46.502970: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 184s 269ms/step - loss: 0.4824 - mean_squared_error: 0.7445 - val_loss: 0.3167 - val_mean_squared_error: 0.7626\n",
    "Epoch 2/5\n",
    "618/618 [==============================] - 167s 264ms/step - loss: 0.2327 - mean_squared_error: 0.7384 - val_loss: 0.1714 - val_mean_squared_error: 0.7622\n",
    "Epoch 3/5\n",
    "618/618 [==============================] - 157s 247ms/step - loss: 0.1374 - mean_squared_error: 0.7383 - val_loss: 0.1123 - val_mean_squared_error: 0.7622\n",
    "Epoch 4/5\n",
    "618/618 [==============================] - 170s 266ms/step - loss: 0.0977 - mean_squared_error: 0.7383 - val_loss: 0.0871 - val_mean_squared_error: 0.7623\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 167s 264ms/step - loss: 0.0804 - mean_squared_error: 0.7383 - val_loss: 0.0758 - val_mean_squared_error: 0.7622\n",
    "\n",
    "\n",
    "Epoch 1/5\n",
    "618/618 [==============================] - 161s 254ms/step - loss: 0.0725 - mean_squared_error: 0.7383 - val_loss: 0.0705 - val_mean_squared_error: 0.7622\n",
    "Epoch 2/5\n",
    "618/618 [==============================] - 166s 257ms/step - loss: 0.0687 - mean_squared_error: 0.7383 - val_loss: 0.0677 - val_mean_squared_error: 0.7622\n",
    "Epoch 3/5\n",
    "618/618 [==============================] - 166s 262ms/step - loss: 0.0667 - mean_squared_error: 0.7383 - val_loss: 0.0663 - val_mean_squared_error: 0.7622\n",
    "Epoch 4/5\n",
    "618/618 [==============================] - 167s 265ms/step - loss: 0.0656 - mean_squared_error: 0.7383 - val_loss: 0.0656 - val_mean_squared_error: 0.7622\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 168s 264ms/step - loss: 0.0651 - mean_squared_error: 0.7382 - val_loss: 0.0653 - val_mean_squared_error: 0.7622\n",
    "\n",
    "\n",
    "Epoch 1/5\n",
    "618/618 [==============================] - 166s 261ms/step - loss: 0.0649 - mean_squared_error: 0.7382 - val_loss: 0.0651 - val_mean_squared_error: 0.7622\n",
    "Epoch 2/5\n",
    "618/618 [==============================] - 163s 253ms/step - loss: 0.0648 - mean_squared_error: 0.7383 - val_loss: 0.0650 - val_mean_squared_error: 0.7622\n",
    "Epoch 3/5\n",
    "618/618 [==============================] - 168s 266ms/step - loss: 0.0647 - mean_squared_error: 0.7383 - val_loss: 0.0650 - val_mean_squared_error: 0.7622\n",
    "Epoch 4/5\n",
    "618/618 [==============================] - 168s 265ms/step - loss: 0.0647 - mean_squared_error: 0.7383 - val_loss: 0.0650 - val_mean_squared_error: 0.7622\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 159s 250ms/step - loss: 0.0647 - mean_squared_error: 0.7382 - val_loss: 0.0650 - val_mean_squared_error: 0.7622"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB29\n",
    "- first to introduce BatchNormalization between every layers.\n",
    "- Initial loss was rather larger than before.\n",
    "- But many epochs later, it began to catch up with NB28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/15\n",
    "2023-05-27 06:26:41.431269: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-27 06:26:42.716711: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 197s 285ms/step - loss: 0.6738 - mean_squared_error: 0.8584 - val_loss: 0.5978 - val_mean_squared_error: 0.7714\n",
    "\n",
    "... ... ...\n",
    "\n",
    "Epoch 15/15\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 176s 279ms/step - loss: 0.0654 - mean_squared_error: 0.7386 - val_loss: 0.0661 - val_mean_squared_error: 0.7641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB30 \n",
    "- Failed to reproduce NB21, as shown in the following cell, because it has the \"standardization\" part, unlike NB21.\n",
    "- Find with \"TRY 21\", for what cells were added to NB29 for from NB30, in order to reproduce NB21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/5\n",
    "2023-05-27 08:30:32.486292: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-27 08:30:33.767349: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 196s 285ms/step - loss: 0.2818 - mean_squared_error: 0.7197 - val_loss: 0.2364 - val_mean_squared_error: 0.7226\n",
    "Epoch 2/5\n",
    "618/618 [==============================] - 186s 294ms/step - loss: 0.2310 - mean_squared_error: 0.7190 - val_loss: 0.2289 - val_mean_squared_error: 0.7227\n",
    "Epoch 3/5\n",
    "618/618 [==============================] - 180s 286ms/step - loss: 0.2282 - mean_squared_error: 0.7190 - val_loss: 0.2282 - val_mean_squared_error: 0.7226\n",
    "Epoch 4/5\n",
    "618/618 [==============================] - 177s 279ms/step - loss: 0.2279 - mean_squared_error: 0.7190 - val_loss: 0.2281 - val_mean_squared_error: 0.7225\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 176s 279ms/step - loss: 0.2279 - mean_squared_error: 0.7190 - val_loss: 0.2281 - val_mean_squared_error: 0.7225"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB31 \n",
    "- succeeded to reproduce NB21, as shown in the following cell, only by removing the \"standardization\" part from NB30.\n",
    "- NB31 keeps \"TRY 21\" cells.\n",
    "- We have to investigate the seemingly successful NB21 and NB31.\n",
    "- If it's a true success, then we need to find out why the removal of \"standardiation\" contributes to reduce losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/5\n",
    "2023-05-27 08:55:23.746663: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-27 08:55:25.032882: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 204s 290ms/step - loss: 0.0321 - mean_squared_error: 0.0036 - val_loss: 7.5261e-04 - val_mean_squared_error: 0.0022\n",
    "Epoch 2/5\n",
    "618/618 [==============================] - 179s 282ms/step - loss: 7.2928e-04 - mean_squared_error: 0.0036 - val_loss: 5.9348e-04 - val_mean_squared_error: 0.0022\n",
    "Epoch 3/5\n",
    "618/618 [==============================] - 179s 283ms/step - loss: 7.0681e-04 - mean_squared_error: 0.0036 - val_loss: 5.9278e-04 - val_mean_squared_error: 0.0022\n",
    "Epoch 4/5\n",
    "618/618 [==============================] - 201s 317ms/step - loss: 7.0670e-04 - mean_squared_error: 0.0036 - val_loss: 5.9294e-04 - val_mean_squared_error: 0.0022\n",
    "Epoch 5/5\n",
    "618/618 [==============================] - 185s 293ms/step - loss: 7.0654e-04 - mean_squared_error: 0.0036 - val_loss: 5.9251e-04 - val_mean_squared_error: 0.0022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB32\n",
    "- Reuse datasets generated by NB31\n",
    "- Remove NB21's 'build_model' and restore NB30's 'build_model, instead, which uses 'BatchNormalization' layer everywhere possible.\n",
    "- The 50th epoch, where learning acually stopped: - 180s 283ms/step - loss: 7.4420e-04 - mean_squared_error: 0.0036 - val_loss: 6.3788e-04 - val_mean_squared_error: 0.0022\n",
    "- During the whole 50 epochs, learning was steady and smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1\n",
    "2023-05-27 10:13:56.356905: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
    "2023-05-27 10:13:57.600543: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "618/618 [==============================] - 198s 286ms/step - loss: 0.2860 - mean_squared_error: 0.1858 - val_loss: 0.2115 - val_mean_squared_error: 0.0348\n",
    "\n",
    "... ... ...\n",
    "\n",
    "Epoch 50/50\n",
    "618/618 [==============================] - 180s 283ms/step - loss: 7.4420e-04 - mean_squared_error: 0.0036 - val_loss: 6.3788e-04 - val_mean_squared_error: 0.0022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The backgdound of the seemingly successful NB21 and NB31\n",
    "- where price is event-freed and standardization was skipped,\n",
    "\n",
    "- EventFree = log(ma(Price)) - MA(log(ma(Price))), where ma is moving average with a small sigma m and MA is with a larger sigma M.\n",
    "- EventFree is essentially logP - MA(logP), if m == 0, and represents the local vibration of logP.\n",
    "- The distribution of EventFree must follow N(0, sigma).\n",
    "    - sigma is small is M is small.\n",
    "    - sigma is small if P is steady.\n",
    "\n",
    "- As EventFree were a very small valuse, and as both X and Y come from EventFree, the losses became very small, even though there was BatchNormalization layers between hidden layers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My mistake\n",
    "\n",
    "### I should have kept the sequence Data[:] in every sample.\n",
    "- k-th sample = { x = Data[k*Ns: k*Ns+Nx, chosen_markets_x, chosen_fileds_x], t = Data[k x Ns + Nx : m x Ns + Nx + Ny, chosen_markets_y, chosen_fileds_y] }\n",
    "### Shuffling/permuting should have been over samples, but not over the time slots inisde a sample.\n",
    "### All NBs through upto NB32 are wrong. Nevertheless, the losses were decreading for many epochs in some NBs. FUNNY.\n",
    "\n",
    "### We will need to create and split a file-streamed, shuffled tf dataset, of unknown size, into train_set and valid_set, before batching and prefetching them.  Possible?\n",
    "- We cannot simply create the dataset as a np array, because it would be of a huge size. (I once thought of this and forgt soon.)\n",
    "\n",
    "### Aha! let's permute and split the list of sample anchors.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a large MA.\n",
    "- Let FreeEvent cross the zero axis over 10 times in a sample (A sample has Nx time slots). We will need a small MA.\n",
    "- Let FreeEvent large enough to ... We will need a large MA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB34\n",
    "- Fixs the mistake of shuffling time spots in samples, rather than samples.\n",
    "- Adds a BatchNorm layer just after the Input layer, while keeping the 'standardization' blocked.\n",
    "- All others are the same as in NB33.\n",
    "- Expected to have much smaller losses, than NB33, because the time slots are now not shuffled, random things but are true stories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "- Change the model to a RNN-based encoder-decoder model.\n",
    "- Change again to attention-based encoder-decoder model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "github.com/tensorflow/tensor2tensor"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
