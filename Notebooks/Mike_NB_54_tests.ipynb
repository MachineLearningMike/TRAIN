{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 09:11:22.861683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 09:11:23.081883: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-11 09:11:24.015270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-11 09:11:24.015406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-11 09:11:24.015417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n",
      "[GCC 9.4.0]\n",
      "matplotlib 3.5.3\n",
      "numpy 1.21.6\n",
      "pandas 1.3.5\n",
      "sklearn 1.0.2\n",
      "tensorflow 2.10.1\n",
      "keras.api._v2.keras 2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mike_NB_tools import *\n",
    "from Transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Hack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_30282/2946334230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mupgrade_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Hack'"
     ]
    }
   ],
   "source": [
    "from Test_Hack import *\n",
    "\n",
    "from Test import *\n",
    "upgrade_file(\"Test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_square' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_30282/3131293039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_square' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3], dtype=np.float32)\n",
    "square = get_square(a)\n",
    "print(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 1234\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001, # default: 0.001 \n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([ [ [0,3,7], [3,0,5], [3,4,5] ], [ [6,0,6], [7,5,3], [5,6,7] ], ], dtype=float)\n",
    "y_pred = tf.constant([ [ [6,23,3], [3,5,2], [5,6,7] ], [ [7,7,10], [8,5,16], [6,7,8] ], ], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossObject = MaskedMSE()\n",
    "loss = lossObject(y_true, y_pred, tf.constant(0.1))\n",
    "print(loss)\n",
    "\n",
    "metricObject = MaskedMSE_Metric()\n",
    "metricObject.update_state(y_true, y_pred, tf.constant(0.1))\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metric = metricObject.result()\n",
    "# metric = metricObject.update_state(y_true, y_pred)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun = MaskedHuber()\n",
    "loss = lossFun(y_true, y_pred, tf.constant(0.1))\n",
    "print(loss)\n",
    "\n",
    "metricObject = MaskedHuber_Metric()\n",
    "metricObject.update_state(y_true, y_pred, tf.constant(0.1))\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metric = metricObject.result()\n",
    "# metric = metricObject.update_state(y_true, y_pred)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_true = tf.constant([ [ [0,3,7], [3,0,5], [3,4,5] ], [ [6,0,6], [7,5,3], [5,6,7] ], ], dtype=float)\n",
    "y_pred = tf.constant([ [ [6,23,3], [3,5,2], [5,6,7] ], [ [7,7,10], [8,5,16], [6,7,8] ], ], dtype=float)\n",
    "\n",
    "mask =   [ [ [ 0,  0,   1], [1,  0,  1] ], [ [1,   0,   1], [ 1,  1,   1] ] ], sum = 8\n",
    "d_true = [ [ [ 3, -3,  -2], [0,  4,  0] ], [ [1,   5,  -3], [-2,  1,   4] ] ]\n",
    "d_pred = [ [ [-3, -18, -1], [2,  1,  5] ], [ [1,  -2,   6], [-2,  2,  -8] ] ]\n",
    "codir =  [ [ [ 0,  1,   1], [0,  1,  0] ], [ [1,   0,   0], [ 1,  1,   0] ] ]\n",
    "masked=  [ [ [ 0,  0,   1], [0,  0,  0] ], [ [1,   0,   0], [ 1,  1,   0] ] ] , sum = 4\n",
    "rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun = MaskedTrendError()\n",
    "loss = lossFun(y_true, y_pred, tf.constant(0.1))\n",
    "print(loss)\n",
    "\n",
    "metricObject = MaskedTrendError_Metric()\n",
    "metricObject.update_state(y_true, y_pred, tf.constant(0.1))\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metric = metricObject.result()\n",
    "# metric = metricObject.update_state(y_true, y_pred)\n",
    "print(metric)\n",
    "\n",
    "metricObject = MaskedTrendAccuracy_Metric()\n",
    "metricObject.update_state(y_true, y_pred, tf.constant(0.1))\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metricObject.update_state(y_true, y_pred)\n",
    "metric = metricObject.result()\n",
    "# metric = metricObject.update_state(y_true, y_pred)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================  Define training parameters ===========================\n",
    "\n",
    "Nx = 600\n",
    "Ny = 5\n",
    "Ns = 5 #--------------------- test\n",
    "BatchSize = 64\n",
    "\n",
    "CandleFile = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "SmallSigma = 1\n",
    "LargeSigma = 30\n",
    "eFreeNoLog = True\n",
    "\n",
    "nFiles_t = 70\n",
    "nFiles_v = 30\n",
    "n_readers = 20\n",
    "shuffle_batch = 200\n",
    "nPrefetch = tf.data.AUTOTUNE\n",
    "\n",
    "dir_datasets = \"/mnt/data/Trading/Datasets\"\n",
    "dir_candles = \"/mnt/data/Trading/Candles\"\n",
    "\n",
    "min_true_candle_percent_x = 40\n",
    "chosen_markets_x = []\n",
    "chosen_fields_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "min_true_candle_percent_y = 40\n",
    "chosen_markets_y = []\n",
    "chosen_fields_y = ['ClosePrice']\n",
    "\n",
    "Standardization = True\n",
    "Time_into_X = True\n",
    "Time_into_Y = True      # True to prevent data contamination\n",
    "Transformer = True\n",
    "Reuse_files = False\n",
    "eFreeNoPlot = False\n",
    "\n",
    "Epochs_Initial = 5\n",
    "\n",
    "Num_Layers = 6\n",
    "Num_Heads = 1\n",
    "Factor_FF = 4\n",
    "repComplexity = 6\n",
    "Dropout_Rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "Candles = np.swapaxes(Candles, 0, 1)\n",
    "print(\"Candles: {}\".format(Candles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[:, market, :], 1, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(Candles[:, market, :], 3, 30, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'Candles'. ====================\n",
    "# Candles.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "CandleMarks = Candles[:, :, 9] # keep it for later use\n",
    "Candles = np.delete(Candles, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, CandleMarks\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "markets = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert Candles.shape[1] == len(markets)\n",
    "print(Candles.shape, len(markets), markets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Restore timestamps_abs. ====================\n",
    "\n",
    "start = datetime( 2000+int(CandleFile[0:2]), int(CandleFile[3:5]), int(CandleFile[6:8]), int(CandleFile[9:11]), int(CandleFile[12:14]) )\n",
    "start_ts = round(datetime.timestamp(start))\n",
    "interval = CandleFile[ CandleFile.find('-', len(CandleFile) - 4) + 1 : ]\n",
    "interval_s = round(intervalToMilliseconds(interval) / 1000)\n",
    "timestamps_abs = np.array( range(start_ts, start_ts + Candles.shape[0] * interval_s, interval_s), dtype=int)\n",
    "assert timestamps_abs.shape[0] == Candles.shape[0]\n",
    "print(start_ts, interval_s, timestamps_abs.shape, timestamps_abs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444., 555.', 2, 4, 2, 2, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11.', 2, 4, 2, 2, True, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 2, 2, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 222., 333., 8., 9., 10., 11., 444.', 2, 4, 1, 4, True, True, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line_to_tensors_for_transformer(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', 2, 4, 1, 4, False, False, 1) # nx, size_x, ny, size_y, time_x, time_y, sixe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defind test data\n",
    "\n",
    "n_times = 1000; n_markets = 2; n_fields = 2\n",
    "data = [ [ [ time * n_markets * n_fields + market * n_fields + field for field in range(n_fields) ] for market in range(n_markets) ] for time in range(n_times)]\n",
    "data = np.array(data, dtype=float)\n",
    "times_test = np.array( range(data.shape[0]) ) + 100000\n",
    "print(data.shape, times_test.shape)   # time, market, field\n",
    "print(data[:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, times_test.shape)   # time, market, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_test = 2\n",
    "ny_test = 2\n",
    "ns_test = 10\n",
    "batchSize = 2\n",
    "\n",
    "sample_anchors = range(0, data.shape[0] - nx_test - ny_test, ns_test)\n",
    "print(data.shape[0], len(sample_anchors), sample_anchors)\n",
    "\n",
    "x_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "y_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "print(data[0:2][:, x_indices[0]][:, :, x_indices[1]])\n",
    "print(data[2:4][:, y_indices[0]][:, :, y_indices[1]])\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "size_time = 1\n",
    "print(size_x, size_y, size_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_plus = CandleFile+'_o'\n",
    "name_prefix = os.path.join(dir_datasets, name_plus)\n",
    "\n",
    "reuse_files = False\n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus))\n",
    "    filenames = divide_to_multiple_csv_files(data, True, True, times_test, sample_anchors, name_prefix, nx_test, x_indices, ny_test, y_indices, header=None, n_parts=10)\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(filenames, shuffle=None) # no way to prevent shuffle.\n",
    "print(filename_dataset.cardinality().numpy())\n",
    "for element in filename_dataset:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(filenames[0])\n",
    "for line in ds.take(20):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename),\n",
    "    cycle_length=n_readers, num_parallel_calls=tf.data.AUTOTUNE) # no way to prevent shuffle?\n",
    "\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_reader_to_dataset(filenames, nx_test, size_x, ny_test, size_y, True, True, size_time,\n",
    "                             n_parse_threads=5, batch_size=batchSize, shuffle_buffer_size=1000, n_readers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    print(element)\n",
    "    break\n",
    "\n",
    "# should print: (None, nx_test, size_x + size_time), (None, ny_test * size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check elements: NaN, -inf, +inf\n",
    "\n",
    "assert (~np.isfinite(Candles)).any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLatest = 500\n",
    "P, maP, logP, log_maP, event, eventFree = Get_eFree(Candles[:, 0, 0], 1, 30, nLatest)\n",
    "assert maP.shape[0] == nLatest; assert logP.shape[0] == nLatest; assert log_maP.shape[0] == nLatest; assert event.shape[0] == nLatest; assert eventFree.shape[0] == nLatest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [\n",
    "    ['APT', 'SUI', 'DYDX', 'ANKR', 'AUDIO', 'SKL'],\n",
    "    ['EOS', 'AAVE', 'FLOW', 'CRV', 'COMP', 'SLP', 'MBOX', 'BNT', 'SPELL', 'AERGO', 'BAKE'],\n",
    "    ['DOT', 'BTC', 'ETH', 'WBTC', 'LINK', 'ETC', 'XLM', 'TWT', 'SFP', 'STPT', 'STEEM', 'POWR'],\n",
    "]\n",
    "\n",
    "for c in range(len(clusters)):\n",
    "    cluster = clusters[c]\n",
    "    cluster = [ markets.index(m + 'USDT') for m in cluster ]\n",
    "    check = [ (markets[m], 100 - round(np.argmax(Candles[:, m, 0]>0) / Candles.shape[0] * 100)) for m in cluster ]\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define Data ====================\n",
    "\n",
    "Data = Candles[:, :, :]   # (time:, all markets, 20 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Find marketrank ==================\n",
    "\n",
    "check = np.array([ np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100 for m in range(len(markets)) ])\n",
    "permute = np.argsort(check)\n",
    "marketrank = [ (markets[m], 100 - round(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) for m in permute ]\n",
    "# marketrank = [ markets[m] for m in permute ]\n",
    "\n",
    "batch = 10\n",
    "for i in range(0, len(markets), batch):\n",
    "    print(marketrank[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Select markets and fields ====================\n",
    "\n",
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "# dot, 1inch, btc, eth, matic, bnb, ada, sol, ltc, avax, wbtc, link, arb, ape, aave, crv, sui, op, gmx, agix, bal, comp, gmt, joe, stg\n",
    "\n",
    "chosen_markets_x = [ elem[0] for elem in marketrank if elem[1] >= min_true_candle_percent_x ]\n",
    "chosen_markets_x = tuple([ markets.index(elem) for elem in chosen_markets_x ])\n",
    "chosen_markets_x = tuple(list(set(chosen_markets_x)))\n",
    "\n",
    "chosen_fields_x = tuple( [ enFields.index(elem) for elem in chosen_fields_x ] )\n",
    "chosen_fields_x = tuple(list(set(chosen_fields_x)))\n",
    "x_indices = ( chosen_markets_x, chosen_fields_x )\n",
    "print(x_indices)\n",
    "\n",
    "chosen_markets_y = [ elem[0] for elem in marketrank if elem[1] >= min_true_candle_percent_y ]\n",
    "chosen_markets_y = tuple([ markets.index(elem) for elem in chosen_markets_y ])\n",
    "chosen_markets_y = tuple(list(set(chosen_markets_y)))\n",
    "\n",
    "chosen_fields_y = tuple( [ enFields.index(elem) for elem in chosen_fields_y ] )\n",
    "chosen_fields_y = tuple(list(set(chosen_fields_y)))\n",
    "y_indices = ( chosen_markets_y, chosen_fields_y )\n",
    "print(y_indices)\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "print(size_x, size_y)\n",
    "\n",
    "chosen_markets = tuple(list(set(chosen_markets_x + chosen_markets_y)))\n",
    "chosen_fields = tuple(list(set(chosen_fields_x + chosen_fields_y)))\n",
    "print(chosen_markets, chosen_fields)\n",
    "\n",
    "print(len(chosen_markets), len(chosen_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_market_names = [markets[market][:-4] for market in chosen_markets]\n",
    "batch = 10\n",
    "for i in range(0, len(chosen_market_names), batch):\n",
    "    print(chosen_market_names[i: i+batch])\n",
    "print(len(chosen_market_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_eFree_with_plot'. Ugly coupling.\n",
    "event_free_data_loss = 3 * ( alpha * SmallSigma + LargeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - event_free_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        sSigma = SmallSigma\n",
    "        if enFields[field] == 'BaseVolume': sSigma = SmallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_eFree_with_plot(markets[market], enFields[field], Data[:, market, field], sSigma, \n",
    "                            LargeSigma, Data.shape[0] - event_free_data_loss, noPlot=eFreeNoPlot, noLog=eFreeNoLog)\n",
    "        Data[event_free_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[event_free_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define time features, to augment Data with ====================\n",
    "\n",
    "sigma = np.power(2.0, -0.2)\n",
    "hourly = np.sin( 2 * np.pi / (60*60) * timestamps_abs ) / sigma\n",
    "daily = np.sin( 2 * np.pi / (60*60*24) * timestamps_abs ) / sigma\n",
    "weekly = np.sin( 2 * np.pi / (60*60*24*7) * timestamps_abs ) / sigma\n",
    "yearly = np.sin( 2 * np.pi / (60*60*24*365) * timestamps_abs ) / sigma\n",
    "\n",
    "Time = np.stack([hourly, daily, weekly, yearly], axis=1)\n",
    "size_time = Time.shape[1]\n",
    "\n",
    "Time = Time[event_free_data_loss: ]\n",
    "assert Data.shape[0] == Time.shape[0]\n",
    "print(Candles.shape, Time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Standardize Data on chosen markets and fields ====================\n",
    "\n",
    "Standard = []\n",
    "\n",
    "if Standardization:\n",
    "    for market in chosen_markets:\n",
    "        for field in chosen_fields:\n",
    "            nzPs = np.where( Data[:, market, field] != 0.0 ) [0]\n",
    "            mu = np.average(Data[nzPs, market, field])\n",
    "            sigma = np.std(Data[nzPs, market, field])\n",
    "            standard = (Data[nzPs, market, field] - mu) / (sigma + 1e-15)\n",
    "            Standard.append( (market, field, mu, sigma) )\n",
    "            Data[nzPs, market, field] = standard\n",
    "\n",
    "    Standard = np.array(Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        ax.plot(Data[:, market, field], label = \"{} @ {}\".format(enFields[field], markets[market][:-4])) # -4: 'USDT'\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define input sequence and output sequence ====================\n",
    "\n",
    "sample_anchors = np.array(range(0, Data.shape[0] - Nx - Ny + 1, Ns))\n",
    "print(Data.shape[0], len(sample_anchors), sample_anchors, sample_anchors[-1])\n",
    "print(Data.shape[0], sample_anchors[ -1 ], sample_anchors[ -1 ] + Nx + Ny, sample_anchors[ -1 ] + Ns, sample_anchors[ -1 ] + Ns + Nx + Ny)\n",
    "\n",
    "for _ in range(10):\n",
    "    permute = np.random.permutation(sample_anchors.shape[0])\n",
    "    sample_anchors = sample_anchors[permute]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sample_anchores_t, sample_anchores_v = train_test_split(sample_anchors, test_size=0.30, random_state=42)\n",
    "print(sample_anchores_t.shape, sample_anchores_v.shape)\n",
    "\n",
    "sample_anchores_t = tuple(sample_anchores_t)\n",
    "sample_anchores_v = tuple(sample_anchores_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Create train/valid datasets ====================\n",
    "\n",
    "name_plus_t = CandleFile+'_t'\n",
    "name_plus_v = CandleFile+'_v'\n",
    "name_prefix_t = os.path.join(dir_datasets, name_plus_t)\n",
    "name_prefix_v = os.path.join(dir_datasets, name_plus_v)\n",
    "\n",
    "reuse_files = Reuse_files #------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames_train = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_t, x)]\n",
    "    filenames_valid = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_v, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_t))\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_v))\n",
    "    filenames_train = divide_to_multiple_csv_files(Data, Time_into_X, Time_into_Y, Time, sample_anchores_t, name_prefix_t, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_t)\n",
    "    filenames_valid = divide_to_multiple_csv_files(Data, Time_into_X, Time_into_Y, Time, sample_anchores_v, name_prefix_v, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_v)\n",
    "\n",
    "# sample_anchores are already shuffled. But we need to shuffle datasets again, because it will reshuffle at every epoch.\n",
    "Dataset_train = csv_reader_to_dataset(filenames_train, Nx, size_x, Ny, size_y, Time_into_X, Time_into_Y, size_time,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*shuffle_batch, n_readers=n_readers, transformer=Transformer)\n",
    "Dataset_train = Dataset_train.prefetch(nPrefetch)\n",
    "\n",
    "Dataset_valid = csv_reader_to_dataset(filenames_valid, Nx, size_x, Ny, size_y, Time_into_X, Time_into_Y, size_time,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*shuffle_batch, n_readers=n_readers, transformer=Transformer)\n",
    "Dataset_valid = Dataset_valid.prefetch(nPrefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y), y_target in Dataset_train.take(1):\n",
    "    break\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "dx = size_x + (size_time if Time_into_X else 0)\n",
    "if Transformer: dx = dx + dx % 2\n",
    "dy = size_y + (size_time if Time_into_Y else 0)\n",
    "if Transformer: dy = dy + dy % 2\n",
    "if Transformer: assert dx == dy\n",
    "\n",
    "cryptoformer = ConTransformer(\n",
    "  num_layers=Num_Layers, d_model=dx, num_heads=Num_Heads, dff=Factor_FF*dx, \n",
    "  repComplexity=repComplexity, target_dim=dy, dropout_rate=Dropout_Rate\n",
    ")\n",
    "\n",
    "cryptoformer.compile(\n",
    "    loss=MaskedMSE,\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.0001,  # def lr = 0.001\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-07\n",
    "    ),\n",
    "    metrics=keras.metrics.MSE,       \n",
    ")\n",
    "\n",
    "# cryptoformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptoformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = ConPositionalEmbedding(complexity=2, d_model=dx, dff=4*dx, dropout_rate=dropout_rate)\n",
    "# p = pos(x)\n",
    "# print(p.shape)\n",
    "# p = pos(y)\n",
    "# print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ConEncoder(num_layers=2, d_model=dx, num_heads=1, dff=4*dx, repComplexity=2, dropout_rate=0.1)\n",
    "# decoder = ConDecoder(num_layers=2, d_model=dy, num_heads=2, dff=4*dy, repComplexity=2, dropout_rate=0.1)\n",
    "# enc = encoder(x)\n",
    "# print(enc.shape)\n",
    "# dec = decoder(x, enc)\n",
    "# print(dec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = len(sample_anchores_t) // BatchSize\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001, # default: 0.001 \n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Fit model ====================\n",
    "\n",
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)\n",
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cryptoformer.fit(Dataset_train, validation_data=Dataset_valid, epochs=Epochs_Initial0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
