{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'intervalToMilliseconds' ====================\n",
    "\n",
    "def intervalToMilliseconds(interval):\n",
    "    \"\"\"Convert a Binance interval string to milliseconds\n",
    "\n",
    "    :param interval: Binance interval string 1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w\n",
    "    :type interval: str\n",
    "\n",
    "    :return:\n",
    "        None if unit not one of m, h, d or w\n",
    "        None if string not in correct format\n",
    "        int value of interval in milliseconds\n",
    "    \"\"\"\n",
    "    ms = None\n",
    "    seconds_per_unit = {\n",
    "        \"m\": 60,\n",
    "        \"h\": 60 * 60,\n",
    "        \"d\": 24 * 60 * 60,\n",
    "        \"w\": 7 * 24 * 60 * 60\n",
    "    }\n",
    "\n",
    "    unit = interval[-1]\n",
    "    if unit in seconds_per_unit:\n",
    "        try:\n",
    "            ms= int(interval[:-1]) * seconds_per_unit[unit] * 1000\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle data into 'table' with shape of (time, markets, 10 fields) ====================\n",
    "\n",
    "# path = \"18-01-01-00-00-23-05-20-05-55-1h\" #\n",
    "path = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "# path = \"18-01-01-00-00-23-05-20-09-11-1d\"\n",
    "\n",
    "table = np.load( os.path.join( \"/mnt/data/Trading/Candles\", \"table-\" + path + \".npy\") )\n",
    "table = np.swapaxes(table, 0, 1)\n",
    "print(\"table: {}\".format(table.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mike_NB_01 import *\n",
    "\n",
    "market = 5\n",
    "Show_Price_Volume_10(table[:, market, :], 1, 1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Free_Learning_Scheme_10(table[:, market, :], 3, 30, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Delete 7 candle fields from 'table'. ====================\n",
    "# table.shape becomes (time, markets, ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume'] )\n",
    "\n",
    "marks = table[:, :, 9] # keep it for later use\n",
    "table = np.delete(table, [0, 1, 2, 5, 6, 8, 9], axis = 2) # delete Open, High, Low, qVolume, #Trades, bQVolume, Marks\n",
    "\n",
    "table_markets = []\n",
    "with open( os.path.join( \"/mnt/data/Trading/Candles\", \"reports-\" + path + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "print(reports[:2])\n",
    "\n",
    "markets = [ s[0: s.find(':')] for s in reports if 'Success' in s ]\n",
    "assert table.shape[1] == len(markets)\n",
    "print(table.shape, len(markets), markets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Restore timestamps. ====================\n",
    "\n",
    "start = datetime( 2000+int(path[0:2]), int(path[3:5]), int(path[6:8]), int(path[9:11]), int(path[12:14]) )\n",
    "start_ts = round(datetime.timestamp(start))\n",
    "interval = path[ path.find('-', len(path) - 4) + 1 : ]\n",
    "interval_s = round(intervalToMilliseconds(interval) / 1000)\n",
    "timestamps = np.array( range(start_ts, start_ts + table.shape[0] * interval_s, interval_s), dtype=int)\n",
    "assert timestamps.shape[0] == table.shape[0]\n",
    "print(start_ts, interval_s, timestamps.shape, timestamps[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find market clusters # temporary\n",
    "# from sklearn.metrics import pairwise\n",
    "\n",
    "# distances = np.zeros( (table.shape[1], table.shape[1]), dtype=float)\n",
    "\n",
    "# # Find dependency distance\n",
    "# for m in range(table.shape[1]):\n",
    "#     distances[m, m] = 0.\n",
    "#     for n in range(m+1, table.shape[1]):\n",
    "#         mask = (marks[:, m] + marks[:, n] == 0) # marks == 0 : true full candles, marks = -1: price interpolated , marks = -2: coincodex prices\n",
    "#         pm = table[mask, m, 0][np.newaxis]\n",
    "#         pn = table[mask, n, 0][np.newaxis]\n",
    "#         distances[m, n] = sklearn.metrics.pairwise.cosine_distances(pm, pn)\n",
    "#         distances[n, m] = distances[m, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import OPTICS\n",
    "# clustering = OPTICS(metric='precomputed', n_jobs=-1).fit(distances)\n",
    "# print( clustering.labels_ )\n",
    "\n",
    "# np.reshape(np.argwhere(clustering.labels_ == 1), -1)\n",
    "\n",
    "# market_clusters = [ [ markets[ id ] for id in np.reshape(np.argwhere(clustering.labels_ == label), -1) ] for label in range(np.max(clustering.labels_))]\n",
    "# print(market_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 0\n",
    "# ids = np.reshape(np.argwhere(clustering.labels_ == cluster), -1)\n",
    "# series = [ [table[:, id, 0], markets[id] ] for id in ids ]\n",
    "# PoltNormalized(\"Market cluster 0. Recent prices are mediocre. Shorter history.\", series, color = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 1\n",
    "# ids = np.reshape(np.argwhere(clustering.labels_ == cluster), -1)\n",
    "# series = [ [table[:, id, 0], markets[id] ] for id in ids ]\n",
    "# PoltNormalized(\"Market cluster 1. Vanished recently. Shorter history, Trends later\", series, color = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 2\n",
    "# ids = np.reshape(np.argwhere(clustering.labels_ == cluster), -1)\n",
    "# series = [ [table[:, id, 0], markets[id] ] for id in ids ]\n",
    "# PoltNormalized(\"Market cluster 2. Not vanished recently. Longer hostory. Trends earlier\", series, color = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'save_to_multiple_csv_files' ====================\n",
    "\n",
    "def save_to_multiple_csv_files(data, sample_anchors, name_prefix, Nx, x_indices, Ny, y_indices, header=None, n_parts=10):\n",
    "    path_format = \"{}_{:02d}.csv\"\n",
    "\n",
    "    filenames = []\n",
    "    for file_idx, anchors in enumerate(np.array_split(sample_anchors, n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for anchor in anchors:\n",
    "                x = np.reshape(data[anchor: anchor + Nx][:, x_indices[0]][:, :, x_indices[1]], -1)\n",
    "                f.write(\",\".join([str(col) for col in x]))\n",
    "                y = np.reshape(data[anchor + Nx: anchor + Nx + Ny][:, y_indices[0]][:, :, y_indices[1]], -1)\n",
    "                f.write(\",\" + \",\".join([str(col) for col in y]))\n",
    "                f.write(\"\\n\")\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'parse_csv_line' ====================\n",
    "\n",
    "def parse_csv_line(line, Nx, size_x, Ny, size_y):\n",
    "    print(line)\n",
    "    defs = [tf.constant(0.0)] * (Nx * size_x + Ny * size_y)\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack( tf.reshape(fields[: Nx * size_x], [Nx, -1] ) )    # sequence of Nx tokens, each of size_x\n",
    "    y = tf.stack(fields[Nx * size_x :])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'csv_reader_dataset' ====================\n",
    "\n",
    "def csv_reader_dataset(filenames, Nx, size_x, Ny, size_y, n_parse_threads=5, batch_size=32, shuffle_buffer_size=32*128, n_readers=5):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    # dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename), #.skip(1), as we have no headers.\n",
    "        cycle_length=n_readers)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)          # Shuffle before batch\n",
    "    dataset = dataset.map(lambda x: parse_csv_line(x, Nx, size_x, Ny, size_y), num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False)   # Batch the shuffled\n",
    "    # dataset = dataset.shuffle(10)          # Shuffle again over batches.\n",
    "    return dataset #.prefetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoint_size(indices):\n",
    "    size = 1\n",
    "    for ids in indices:\n",
    "        size *= len(ids)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defind test data\n",
    "\n",
    "n_times = 1000; n_markets = 2; n_fields = 2\n",
    "data = [ [ [ time * n_markets * n_fields + market * n_fields + field for field in range(n_fields) ] for market in range(n_markets) ] for time in range(n_times)]\n",
    "data = np.array(data, dtype=float)\n",
    "print(data.shape)   # time, market, field\n",
    "print(data[:2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 2\n",
    "Ny = 2\n",
    "Ns = 10\n",
    "BatchSize = 2\n",
    "\n",
    "sample_anchors = range(0, data.shape[0] - Nx - Ny, Ns)\n",
    "print(data.shape[0], len(sample_anchors), sample_anchors)\n",
    "\n",
    "x_indices = ( (0, 1), (0, 1) )    # (market, field)\n",
    "y_indices = ( (0,), (0, 1) )    # (market, field)\n",
    "print(data[0:2][:, x_indices[0]][:, :, x_indices[1]])\n",
    "print(data[2:4][:, y_indices[0]][:, :, y_indices[1]])\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "print(size_x, size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_datasets = \"/mnt/data/Trading/Datasets\"\n",
    "name_plus = path+'_o'\n",
    "name_prefix = os.path.join(dir_datasets, name_plus)\n",
    "\n",
    "reuse_files = True\n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus))\n",
    "    filenames = save_to_multiple_csv_files(data, sample_anchors, name_prefix, Nx, x_indices, Ny, y_indices, header=None, n_parts=10)\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(filenames, shuffle=None) # no way to prevent shuffle.\n",
    "print(filename_dataset.cardinality().numpy())\n",
    "for element in filename_dataset:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(filenames[0])\n",
    "for line in ds.take(20):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename),\n",
    "    cycle_length=n_readers, num_parallel_calls=tf.data.AUTOTUNE) # no way to prevent shuffle?\n",
    "\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_csv_line(b'0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.', Nx, size_x, Ny, size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_reader_dataset(filenames, Nx, size_x, Ny, size_y,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=100000, n_readers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    print(element)\n",
    "    break\n",
    "\n",
    "# should print: (None, Nx, size_x), (None, Ny * size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check elements: NaN, -inf, +inf\n",
    "\n",
    "assert (~np.isfinite(table)).any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'Get_Event_Free_Feature' ====================\n",
    "\n",
    "def Get_Event_Free_Feature(feature, smallSigma, largeSigma, nLatest):\n",
    "\n",
    "    def gaussian( x, s): return 1./np.sqrt( 2. * np.pi * s**2 ) * np.exp( -x**2 / ( 2. * s**2 ) )\n",
    "\n",
    "    smallSigma = min(math.floor(feature.shape[0]/3), smallSigma)\n",
    "    smallP = 3 * smallSigma\n",
    "    smallKernel = np.fromiter( (gaussian( x , smallSigma ) for x in range(-smallP+1, 1, 1 ) ), float ) # smallP points, incl 0.\n",
    "#     print(\"smallKernel: {}\".format(smallKernel))\n",
    "    maP = np.convolve(feature, smallKernel, mode=\"valid\") / np.sum(smallKernel) # maps to feature[smallP-1:]\n",
    "\n",
    "    # maP = maP / np.min(1.0, np.min(maP[np.where(maP>0.0)]))\n",
    "    nzPs = np.where(maP > 0.0)[0] [smallP:]    # to exclude initial nearly-zero values.\n",
    "    log_maP = np.zeros( maP.shape, dtype=maP.dtype)\n",
    "    log_maP[nzPs] = np.log2(maP[nzPs])  #------------------------------------------ Log danger ------------\n",
    "\n",
    "    # log_maP = np.log2(maP + 1e-9) # maps to feature[smallP-1:]\n",
    "\n",
    "    largeSigma = min(math.floor(feature.shape[0]/3), largeSigma)\n",
    "    largeP = 3 * largeSigma\n",
    "    largeKernel = np.fromiter( (gaussian( x , largeSigma ) for x in range(-largeP+1, 1, 1 ) ), float ) # largeP points, incl 0.\n",
    "#     print(\"largeKernel: {}\".format(largeKernel))\n",
    "    event = np.convolve(log_maP, largeKernel, mode=\"valid\") / np.sum(largeKernel) # maps to log_maP[largeP-1:], so to feature[smallP+largeP-2:]\n",
    "\n",
    "    assert event.shape[0] == feature.shape[0] - (smallP+largeP-2)\n",
    "    log_maP1 = log_maP[largeP-1:] # maps to log_maP[largeP-1:], so to feature[smalP+largeP-2:]\n",
    "    assert log_maP1.shape[0] == feature.shape[0] - (smallP+largeP-2)\n",
    "    P1 = feature[smallP+largeP-2:]\n",
    "    assert P1.shape[0] == feature.shape[0] - (smallP+largeP-2)\n",
    "    eventFree = log_maP1 - event # maps to feature[smallP+largeP-2:]\n",
    "\n",
    "    nLatest = min(feature.shape[0] - (smallP+largeP-2), nLatest)\n",
    "    P2 = P1[-nLatest:]\n",
    "    maP2 = maP[-nLatest:]\n",
    "\n",
    "    # P2 = P2 / np.min(1.0, np.min(P2[np.where(P2>0.0)]))\n",
    "    nzPs = np.where(P2 > 0.0) [0] [:]\n",
    "    logP2 = np.zeros( P2.shape, dtype=P2.dtype)\n",
    "    logP2[nzPs] = np.log2(P2[nzPs]) #------------------------------------------ Log danger ------------\n",
    "\n",
    "    # logP2 = np.log2(P2 + 1e-9) \n",
    "    \n",
    "    log_maP2 = log_maP1[-nLatest:]\n",
    "    event2 = event[-nLatest:]\n",
    "    eventFree2 = eventFree[-nLatest:] # maps to candle[p1-1+p2-1+begin: p1-1+p2-1+begine+width]\n",
    "\n",
    "    return P2, maP2, logP2, log_maP2, event2, eventFree2    # eventFree = log_maP - event, event = convolve(lag_maP, leftKernel) / sum(leftKernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nLatest = 500\n",
    "P, maP, logP, log_maP, event, eventFree = Get_Event_Free_Feature(table[:, 0, 0], 1, 30, nLatest)\n",
    "assert maP.shape[0] == nLatest; assert logP.shape[0] == nLatest; assert log_maP.shape[0] == nLatest; assert event.shape[0] == nLatest; assert eventFree.shape[0] == nLatest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'get_plot_log_feature' ====================\n",
    "\n",
    "def get_plot_log_feature(market, field, feature, smallSigma, largeSigma, nLatest, NoChart = True):\n",
    "    P, maP, logP, log_maP, event, eventFree = Get_Event_Free_Feature(feature, smallSigma, largeSigma, nLatest)\n",
    "    series = [ [maP, \"maP\", \"g\"], [logP, \"logP\" ,\"m\"], [log_maP, \"log.maP\", \"b\"], [event, \"event\", \"c\"],  [eventFree, \"e.Free\", \"brown\"] ] #, [P, \"raw feature\", \"r\"] ]\n",
    "    if not NoChart:\n",
    "        PoltNormalized(\"Event-free (brown) {} on {}\".format(field, market), series)\n",
    "    return P, maP, logP, log_maP, event, eventFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markets orderd in the mount of true candles they have.\n",
    "# ['NEOUSDT', 'LTCUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'QTUMUSDT', 'ADAUSDT', 'XRPUSDT', 'EOSUSDT', 'XLMUSDT', 'IOTAUSDT', 'ONTUSDT', 'TRXUSDT', 'ETCUSDT', 'ICXUSDT']\n",
    "# ['NULSUSDT', 'VETUSDT', 'LINKUSDT', 'WAVESUSDT', 'ONGUSDT', 'HOTUSDT', 'ZILUSDT', 'ZRXUSDT', 'FETUSDT', 'BATUSDT', 'XMRUSDT', 'ZECUSDT', 'IOSTUSDT', 'CELRUSDT', 'DASHUSDT']\n",
    "# ['OMGUSDT', 'THETAUSDT', 'ENJUSDT', 'MATICUSDT', 'ATOMUSDT', 'TFUELUSDT', 'ONEUSDT', 'FTMUSDT', 'ALGOUSDT', 'DOGEUSDT', 'DUSKUSDT', 'ANKRUSDT', 'WINUSDT', 'COSUSDT', 'COCOSUSDT']\n",
    "# ['MTLUSDT', 'TOMOUSDT', 'PERLUSDT', 'KEYUSDT', 'WANUSDT', 'FUNUSDT', 'DENTUSDT', 'DOCKUSDT', 'CHZUSDT', 'BANDUSDT', 'BUSDUSDT', 'XTZUSDT', 'RVNUSDT', 'RENUSDT', 'HBARUSDT']\n",
    "# ['NKNUSDT', 'KAVAUSDT', 'STXUSDT', 'ARPAUSDT', 'IOTXUSDT', 'RLCUSDT', 'CTXCUSDT', 'BCHUSDT', 'TROYUSDT', 'VITEUSDT', 'OGNUSDT', 'DREPUSDT', 'WRXUSDT', 'BTSUSDT', 'LSKUSDT']\n",
    "# ['BNTUSDT', 'LTOUSDT', 'MBLUSDT', 'COTIUSDT', 'STPTUSDT', 'WTCUSDT', 'DATAUSDT', 'CTSIUSDT', 'HIVEUSDT', 'CHRUSDT', 'ARDRUSDT', 'MDTUSDT', 'STMXUSDT', 'KNCUSDT', 'LRCUSDT']\n",
    "# ['PNTUSDT', 'COMPUSDT', 'ZENUSDT', 'SCUSDT', 'SNXUSDT', 'VTHOUSDT', 'DGBUSDT', 'SXPUSDT', 'MKRUSDT', 'DCRUSDT', 'STORJUSDT', 'MANAUSDT', 'YFIUSDT', 'KMDUSDT', 'IRISUSDT']\n",
    "# ['SOLUSDT', 'BLZUSDT', 'BALUSDT', 'JSTUSDT', 'ANTUSDT', 'SANDUSDT', 'CRVUSDT', 'DOTUSDT', 'OCEANUSDT', 'NMRUSDT', 'LUNAUSDT', 'RSRUSDT', 'WNXMUSDT', 'PAXGUSDT', 'TRBUSDT']\n",
    "# ['SUSHIUSDT', 'YFIIUSDT', 'EGLDUSDT', 'FIOUSDT', 'RUNEUSDT', 'KSMUSDT', 'DIAUSDT', 'UMAUSDT', 'BELUSDT', 'WINGUSDT', 'UNIUSDT', 'OXTUSDT', 'SUNUSDT', 'AVAXUSDT', 'FLMUSDT']\n",
    "# ['ORNUSDT', 'UTKUSDT', 'XVSUSDT', 'ALPHAUSDT', 'NEARUSDT', 'AAVEUSDT', 'FILUSDT', 'INJUSDT', 'AUDIOUSDT', 'CTKUSDT', 'AKROUSDT', 'AXSUSDT', 'HARDUSDT', 'STRAXUSDT', 'UNFIUSDT']\n",
    "# ['ROSEUSDT', 'AVAUSDT', 'XEMUSDT', 'SKLUSDT', 'GRTUSDT', 'JUVUSDT', 'PSGUSDT', '1INCHUSDT', 'REEFUSDT', 'ASRUSDT', 'ATMUSDT', 'OGUSDT', 'CELOUSDT', 'RIFUSDT', 'TRUUSDT']\n",
    "# ['CKBUSDT', 'TWTUSDT', 'FIROUSDT', 'LITUSDT', 'SFPUSDT', 'CAKEUSDT', 'DODOUSDT', 'ACMUSDT', 'BADGERUSDT', 'FISUSDT', 'OMUSDT', 'PONDUSDT', 'DEGOUSDT', 'ALICEUSDT', 'LINAUSDT']\n",
    "# ['PERPUSDT', 'SUPERUSDT', 'CFXUSDT', 'TKOUSDT', 'PUNDIXUSDT', 'TLMUSDT', 'BARUSDT', 'FORTHUSDT', 'BURGERUSDT', 'SLPUSDT', 'BAKEUSDT', 'SHIBUSDT', 'ICPUSDT', 'ARUSDT', 'POLSUSDT']\n",
    "# ['MDXUSDT', 'MASKUSDT', 'LPTUSDT', 'XVGUSDT', 'ATAUSDT', 'GTCUSDT', 'ERNUSDT', 'KLAYUSDT', 'PHAUSDT', 'MLNUSDT', 'BONDUSDT', 'DEXEUSDT', 'C98USDT', 'CLVUSDT', 'QNTUSDT']\n",
    "# ['FLOWUSDT', 'TVKUSDT', 'MINAUSDT', 'RAYUSDT', 'ALPACAUSDT', 'FARMUSDT', 'QUICKUSDT', 'MBOXUSDT', 'REQUSDT', 'FORUSDT', 'GHSTUSDT', 'WAXPUSDT', 'GNOUSDT', 'XECUSDT', 'ELFUSDT']\n",
    "# ['DYDXUSDT', 'IDEXUSDT', 'VIDTUSDT', 'GALAUSDT', 'ILVUSDT', 'YGGUSDT', 'SYSUSDT', 'DFUSDT', 'FIDAUSDT', 'FRONTUSDT', 'CVPUSDT', 'AGLDUSDT', 'RADUSDT', 'BETAUSDT', 'RAREUSDT']\n",
    "# ['LAZIOUSDT', 'CHESSUSDT', 'ADXUSDT', 'AUCTIONUSDT', 'DARUSDT', 'BNXUSDT', 'MOVRUSDT', 'ENSUSDT', 'CITYUSDT', 'KP3RUSDT', 'QIUSDT', 'PORTOUSDT', 'POWRUSDT', 'VGXUSDT', 'JASMYUSDT']\n",
    "# ['PLAUSDT', 'AMPUSDT', 'PYRUSDT', 'RNDRUSDT', 'ALCXUSDT', 'SANTOSUSDT', 'MCUSDT', 'BICOUSDT', 'FLUXUSDT', 'FXSUSDT', 'VOXELUSDT', 'HIGHUSDT', 'PEOPLEUSDT', 'CVXUSDT', 'OOKIUSDT']\n",
    "# ['SPELLUSDT', 'JOEUSDT', 'IMXUSDT', 'ACHUSDT', 'GLMRUSDT', 'LOKAUSDT', 'API3USDT', 'SCRTUSDT', 'ACAUSDT', 'XNOUSDT', 'WOOUSDT', 'ALPINEUSDT', 'TUSDT', 'ASTRUSDT', 'GMTUSDT']\n",
    "# ['KDAUSDT', 'APEUSDT', 'BSWUSDT', 'BIFIUSDT', 'MULTIUSDT', 'STEEMUSDT', 'MOBUSDT', 'NEXOUSDT', 'REIUSDT', 'GALUSDT', 'LDOUSDT', 'EPXUSDT', 'OPUSDT', 'LEVERUSDT', 'STGUSDT']\n",
    "# ['LUNCUSDT', 'GMXUSDT', 'POLYXUSDT', 'APTUSDT', 'OSMOUSDT', 'HFTUSDT', 'PHBUSDT', 'HOOKUSDT', 'MAGICUSDT', 'HIFIUSDT', 'RPLUSDT', 'PROSUSDT', 'AGIXUSDT', 'GNSUSDT', 'SYNUSDT']\n",
    "# ['SSVUSDT', 'VIBUSDT', 'LQTYUSDT', 'AMBUSDT', 'USTCUSDT', 'UFTUSDT', 'PROMUSDT', 'GLMUSDT', 'GASUSDT', 'QKCUSDT', 'IDUSDT', 'ARBUSDT', 'OAXUSDT', 'LOOMUSDT', 'RDNTUSDT']\n",
    "# ['WBTCUSDT', 'EDUUSDT', 'SUIUSDT', 'AERGOUSDT', 'FLOKIUSDT', 'PEPEUSDT']\n",
    "\n",
    "# Market clusters\n",
    "# 0: [('APTUSDT', 11), ('SUIUSDT', 1), ('DYDXUSDT', 31), ('ANKRUSDT', 71), ('AUDIOUSDT', 48), ('SKLUSDT', 46)]\n",
    "# 1: [('EOSUSDT', 93), ('AAVEUSDT', 48), ('FLOWUSDT', 33), ('CRVUSDT', 51), ('COMPUSDT', 54), ('SLPUSDT', 38), ('MBOXUSDT', 32), ('BNTUSDT', 61), ('SPELLUSDT', 26), ('AERGOUSDT', 1), ('BAKEUSDT', 38)]\n",
    "# 2: [('DOTUSDT', 51), ('BTCUSDT', 100), ('ETHUSDT', 100), ('WBTCUSDT', 1), ('LINKUSDT', 81), ('ETCUSDT', 92), ('XLMUSDT', 92), ('TWTUSDT', 43), ('SFPUSDT', 42), ('STPTUSDT', 59), ('STEEMUSDT', 20), ('POWRUSDT', 28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [\n",
    "    ['APT', 'SUI', 'DYDX', 'ANKR', 'AUDIO', 'SKL'],\n",
    "    ['EOS', 'AAVE', 'FLOW', 'CRV', 'COMP', 'SLP', 'MBOX', 'BNT', 'SPELL', 'AERGO', 'BAKE'],\n",
    "    ['DOT', 'BTC', 'ETH', 'WBTC', 'LINK', 'ETC', 'XLM', 'TWT', 'SFP', 'STPT', 'STEEM', 'POWR'],\n",
    "]\n",
    "\n",
    "for c in range(len(clusters)):\n",
    "    cluster = clusters[c]\n",
    "    cluster = [ markets.index(m + 'USDT') for m in cluster ]\n",
    "    check = [ (markets[m], 100 - round(np.argmax(table[:, m, 0]>0) / table.shape[0] * 100)) for m in cluster ]\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Select markets and fields ====================\n",
    "\n",
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "# dot, 1inch, btc, eth, matic, bnb, ada, sol, ltc, avax, wbtc, link, arb, ape, aave, crv, sui, op, gmx, agix, bal, comp, gmt, joe, stg\n",
    "chosen_markets_x = [\n",
    "                    'DOTUSDT', 'BTCUSDT', 'ETHUSDT', 'LINKUSDT', 'ETCUSDT', 'XLMUSDT', 'STPTUSDT',   # cluster 2, with over 50% true candles, except 'DOTUSDT'\n",
    "                    'EOSUSDT', 'BNTUSDT', # cluster 1, with over 50% true candles.\n",
    "                    'ANKRUSDT', # cluster 0, with over 50% true candles.\n",
    "                    'BNBUSDT', 'AVAXUSDT', 'COMPUSDT', 'BALUSDT', 'XRPUSDT', '1INCHUSDT'  # etc\n",
    "]\n",
    "chosen_markets_x = tuple([ markets.index(elem) for elem in chosen_markets_x ])\n",
    "chosen_markets_x = tuple(list(set(chosen_markets_x)))\n",
    "chosen_fields_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "chosen_fields_x = tuple( [ enFields.index(elem) for elem in chosen_fields_x ] )\n",
    "chosen_fields_x = tuple(list(set(chosen_fields_x)))\n",
    "x_indices = ( chosen_markets_x, chosen_fields_x )\n",
    "print(x_indices)\n",
    "\n",
    "chosen_markets_y = [\n",
    "                    'DOTUSDT', 'BTCUSDT', 'ETHUSDT', 'LINKUSDT', 'ETCUSDT', 'XLMUSDT', 'STPTUSDT',   # cluster 2, with over 50% true candles, except 'DOTUSDT'\n",
    "                    'EOSUSDT', 'BNTUSDT', # cluster 1, with over 50% true candles.\n",
    "                     'ANKRUSDT', # cluster 0, with over 50% true candles.\n",
    "                     'BNBUSDT', 'AVAXUSDT', 'COMPUSDT', 'BALUSDT', 'XRPUSDT', '1INCHUSDT'  # etc\n",
    "]\n",
    "chosen_markets_y = tuple([ markets.index(elem) for elem in chosen_markets_y ])\n",
    "chosen_markets_y = tuple(list(set(chosen_markets_y)))\n",
    "chosen_fields_y = ['ClosePrice']\n",
    "chosen_fields_y = tuple( [ enFields.index(elem) for elem in chosen_fields_y ] )\n",
    "chosen_fields_y = tuple(list(set(chosen_fields_y)))\n",
    "y_indices = ( chosen_markets_y, chosen_fields_y )\n",
    "print(y_indices)\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "print(size_x, size_y)\n",
    "\n",
    "chosen_markets = tuple(list(set(chosen_markets_x + chosen_markets_y)))\n",
    "chosen_fields = tuple(list(set(chosen_fields_x + chosen_fields_y)))\n",
    "print(chosen_markets, chosen_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 21\n",
    "\n",
    "# Define Data\n",
    "enFields = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "\n",
    "# dot, 1inch, btc, eth, matic, bnb, ada, sol, ltc, avax, wbtc, link, arb, ape, aave, crv, sui, op, gmx, agix, bal, comp, gmt, joe, stg\n",
    "chosen_markets_x = ['DOTUSDT', '1INCHUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'LTCUSDT', 'AVAXUSDT', 'WBTCUSDT', 'LINKUSDT',\n",
    "                  'ARBUSDT', 'APEUSDT', 'SUIUSDT', 'OPUSDT', 'GMXUSDT', 'AGIXUSDT', 'BALUSDT', 'COMPUSDT', 'GMTUSDT', 'JOEUSDT', 'STGUSDT']\n",
    "chosen_markets_x = tuple([ markets.index(elem) for elem in chosen_markets_x ])\n",
    "chosen_markets_x = tuple(list(set(chosen_markets_x)))\n",
    "chosen_fields_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "chosen_fields_x = tuple( [ enFields.index(elem) for elem in chosen_fields_x ] )\n",
    "chosen_fields_x = tuple(list(set(chosen_fields_x)))\n",
    "x_indices = ( chosen_markets_x, chosen_fields_x )\n",
    "print(x_indices)\n",
    "\n",
    "chosen_markets_y = ['DOTUSDT', '1INCHUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'LTCUSDT', 'AVAXUSDT', 'WBTCUSDT', 'LINKUSDT', ]\n",
    "chosen_markets_y = tuple([ markets.index(elem) for elem in chosen_markets_y ])\n",
    "chosen_markets_y = tuple(list(set(chosen_markets_y)))\n",
    "chosen_fields_y = ['ClosePrice']\n",
    "chosen_fields_y = tuple( [ enFields.index(elem) for elem in chosen_fields_y ] )\n",
    "chosen_fields_y = tuple(list(set(chosen_fields_y)))\n",
    "y_indices = ( chosen_markets_y, chosen_fields_y )\n",
    "print(y_indices)\n",
    "\n",
    "size_x = get_timepoint_size(x_indices)\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "print(size_x, size_y)\n",
    "\n",
    "chosen_markets = tuple(list(set(chosen_markets_x + chosen_markets_y)))\n",
    "chosen_fields = tuple(list(set(chosen_fields_x + chosen_fields_y)))\n",
    "print(chosen_markets, chosen_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define Data ====================\n",
    "\n",
    "Data = table[:, :, :]   # (time:, all markets, 20 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = [ (markets[m], \"{}%\".format(100-int(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) ) for m in chosen_markets ]\n",
    "batch = 5\n",
    "for i in range(0, len(check), batch):\n",
    "    print(check[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.array([ np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100 for m in range(len(markets)) ])\n",
    "permute = np.argsort(check)\n",
    "marketrank = [ (markets[m], 100 - round(np.argmax(Data[:, m, 0]>0) / Data.shape[0] * 100)) for m in permute ]\n",
    "# marketrank = [ markets[m] for m in permute ]\n",
    "\n",
    "batch = 10\n",
    "for i in range(0, len(markets), batch):\n",
    "    print(marketrank[i: i+batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Generate event-free data into Data ====================\n",
    "# Data loses heading items.\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "smallSigma = 1\n",
    "largeSigma = 30\n",
    "\n",
    "alpha = 3; beta = 3 # beta is used in 'get_plot_log_feature'. Ugly coupling.\n",
    "head_data_loss = 3 * ( alpha * smallSigma + largeSigma)\n",
    "eFree = np.zeros( (Data.shape[0] - head_data_loss, len(chosen_markets), len(chosen_fields)), dtype = np.float32 )\n",
    "\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        sSigma = smallSigma\n",
    "        if enFields[field] == 'BaseVolume': sSigma = smallSigma * alpha\n",
    "        P, maP, logP, log_maP, event, eventFree = \\\n",
    "        get_plot_log_feature(markets[market], enFields[field], Data[:, market, field], sSigma, largeSigma, Data.shape[0] - head_data_loss, NoChart = False)\n",
    "        Data[head_data_loss:, market, field] = eventFree\n",
    "\n",
    "Data = Data[head_data_loss: ]\n",
    "\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim in time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 21\n",
    "\n",
    "# #==================== Standardize Data on chosen markets and fields ====================\n",
    "# # No need to Do it before: Permute Data in time\n",
    "\n",
    "# for market in chosen_markets:\n",
    "#     for field in chosen_fields:\n",
    "#         nzPs = np.where( Data[:, market, field] != 0.0 ) [0]\n",
    "#         # Keep record of mu and sigma for later use with inference. ----------------------------------\n",
    "#         mu = np.average(Data[nzPs, market, field])\n",
    "#         sigma = np.std(Data[nzPs, market, field])\n",
    "#         standard = (Data[nzPs, market, field] - mu) / (sigma + 1e-9)\n",
    "#         Data[nzPs, market, field] = standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized.\")\n",
    "for market in chosen_markets:\n",
    "    for field in chosen_fields:\n",
    "        ax.plot(Data[:, market, field], label = \"{} @ {}\".format(enFields[field], markets[market][:-4])) # -4: 'USDT'\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define time features, to augment Data with ====================\n",
    "# Do it before: Permute Data in time\n",
    "\n",
    "hourly = np.sin( 2 * np.pi / (60*60) * timestamps )\n",
    "daily = np.sin( 2 * np.pi / (60*60*24) * timestamps )\n",
    "weekly = np.sin( 2 * np.pi / (60*60*24*7) * timestamps )\n",
    "yearly = np.sin( 2 * np.pi / (60*60*24*365) * timestamps )\n",
    "\n",
    "\n",
    "# A normalized representation of 'timestamps'\n",
    "time_of_year_sin = np.sign( 2 * np.pi / (60*60*24*365) * timestamps )\n",
    "time_of_year_cos = np.cos( 2 * np.pi / (60*60*24*365) * timestamps )\n",
    "time_of_week_sin = np.sign( 2 * np.pi / (60*60*24*7) * timestamps )\n",
    "time_of_week_cos = np.cos( 2 * np.pi / (60*60*24*7) * timestamps )\n",
    "time_of_day_sin = np.sign( 2 * np.pi / (60*60*24) * timestamps )\n",
    "time_of_day_cos = np.cos( 2 * np.pi / (60*60*24) * timestamps )\n",
    "\n",
    "print(table.shape, hourly.shape)\n",
    "\n",
    "# table = np.insert(table, 0, hourly.reshape(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Permute Data in time, and split into train/valid part ====================\n",
    "\n",
    "permute = np.random.permutation(len(Data)); Data = Data[permute]\n",
    "permute = np.random.permutation(len(Data)); Data = Data[permute]\n",
    "permute = np.random.permutation(len(Data)); Data = Data[permute]\n",
    "print(Data.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Train, Valid = train_test_split(Data, test_size=0.30, random_state=42)\n",
    "print(Train.shape, Valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define input sequence and output sequence ====================\n",
    "\n",
    "Nx = 500\n",
    "Ny = 10\n",
    "Ns = 10\n",
    "BatchSize = 64\n",
    "\n",
    "sample_anchors_t = range(0, Train.shape[0] - Nx - Ny + 1, Ns)\n",
    "print(Train.shape[0], len(sample_anchors_t), sample_anchors_t, sample_anchors_t[-1])\n",
    "print(Data.shape[0], sample_anchors_t[ -1 ], sample_anchors_t[ -1 ] + Nx + Ny, sample_anchors_t[ -1 ] + Ns, sample_anchors_t[ -1 ] + Ns + Nx + Ny)\n",
    "\n",
    "sample_anchors_v = range(0, Valid.shape[0] - Nx - Ny + 1, Ns)\n",
    "print(Valid.shape[0], len(sample_anchors_v), sample_anchors_v, sample_anchors_v[-1])\n",
    "print(Data.shape[0], sample_anchors_v[ -1 ], sample_anchors_v[ -1 ] + Nx + Ny, sample_anchors_v[ -1 ] + Ns, sample_anchors_v[ -1 ] + Ns + Nx + Ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 21\n",
    "\n",
    "Nx = 500\n",
    "Ny = 5\n",
    "Ns = 10\n",
    "BatchSize = 64\n",
    "\n",
    "sample_anchors_t = range(0, Train.shape[0] - Nx - Ny + 1, Ns)\n",
    "print(Train.shape[0], len(sample_anchors_t), sample_anchors_t, sample_anchors_t[-1])\n",
    "print(Data.shape[0], sample_anchors_t[ -1 ], sample_anchors_t[ -1 ] + Nx + Ny, sample_anchors_t[ -1 ] + Ns, sample_anchors_t[ -1 ] + Ns + Nx + Ny)\n",
    "\n",
    "sample_anchors_v = range(0, Valid.shape[0] - Nx - Ny + 1, Ns)\n",
    "print(Valid.shape[0], len(sample_anchors_v), sample_anchors_v, sample_anchors_v[-1])\n",
    "print(Data.shape[0], sample_anchors_v[ -1 ], sample_anchors_v[ -1 ] + Nx + Ny, sample_anchors_v[ -1 ] + Ns, sample_anchors_v[ -1 ] + Ns + Nx + Ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Create train/valid datasets ====================\n",
    "\n",
    "nFiles_t = 70\n",
    "nFiles_v = 30\n",
    "n_readers = 10\n",
    "suffule_batch = 300\n",
    "prefetch = 3\n",
    "\n",
    "dir_datasets = \"/mnt/data/Trading/Datasets/\"\n",
    "\n",
    "name_plus_t = path+'_t'\n",
    "name_plus_v = path+'_v'\n",
    "name_prefix_t = os.path.join(dir_datasets, name_plus_t)\n",
    "name_prefix_v = os.path.join(dir_datasets, name_plus_v)\n",
    "\n",
    "reuse_files = True #------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "if reuse_files:\n",
    "    import re\n",
    "    filenames_train = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_t, x)]\n",
    "    filenames_valid = [ os.path.join(dir_datasets, x) for x in os.listdir(dir_datasets) if re.match(name_plus_v, x)]\n",
    "else:\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_t))\n",
    "    os.system(\"rm {}/*{}*\".format(dir_datasets, name_plus_v))\n",
    "    filenames_train = save_to_multiple_csv_files(Train, sample_anchors_t, name_prefix_t, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_t)\n",
    "    filenames_valid = save_to_multiple_csv_files(Valid, sample_anchors_v, name_prefix_v, Nx, x_indices, Ny, y_indices, header=None, n_parts=nFiles_v)\n",
    "\n",
    "Dataset_train = csv_reader_dataset(filenames_train, Nx, size_x, Ny, size_y,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*suffule_batch, n_readers=n_readers)\n",
    "Dataset_train = Dataset_train.prefetch(prefetch)\n",
    "# print(len(Dataset_train))\n",
    "\n",
    "Dataset_valid = csv_reader_dataset(filenames_valid, Nx, size_x, Ny, size_y,\n",
    "                             n_parse_threads=5, batch_size=BatchSize, shuffle_buffer_size=BatchSize*suffule_batch, n_readers=n_readers)\n",
    "Dataset_valid = Dataset_valid.prefetch(prefetch)\n",
    "# print(len(Dataset_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in Dataset_train:\n",
    "    print(element)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_last_step(Y_true, Y_pred):\n",
    "    return keras.mean(keras.abs(Y_pred[:, -1] - Y_true[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define 'huber_loss' ====================\n",
    "\n",
    "def huber_loss(y_true, y_pred, max_grad=0.2):   # default: max_grad = 1.0\n",
    "    err = tf.abs(y_true - y_pred, name='abs')\n",
    "    mg = tf.constant(max_grad, name='max_grad')\n",
    "    lin = mg * (err - 0.5 * mg)\n",
    "    quad = 0.5 * err * err\n",
    "    return tf.where(err < mg, quad, lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 21\n",
    "\n",
    "def huber_loss(y_true, y_pred, max_grad=1.):\n",
    "    err = tf.abs(y_true - y_pred, name='abs')\n",
    "    mg = tf.constant(max_grad, name='max_grad')\n",
    "    lin = mg * (err - 0.5 * mg)\n",
    "    quad = 0.5 * err * err\n",
    "    return tf.where(err < mg, quad, lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define RegularizedLSTM ====================\n",
    "\n",
    "from functools import partial\n",
    "RegularizedLSTM = partial(keras.layers.LSTM,\n",
    "                          return_sequences=True,\n",
    "                          kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                          recurrent_regularizer=keras.regularizers.l2(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define build_model ====================\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_size, allow_cudnn_kernel=True):\n",
    "\n",
    "    units = max(input_dim, output_size)\n",
    "\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "\n",
    "        inputs = keras.Input( shape = (None, input_dim), name = \"candles\" )\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(inputs)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = RegularizedLSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.LSTM(units, kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        x = keras.layers.Dense(units, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "        x = keras.layers.BatchNormalization(synchronized=True)(x)\n",
    "        outputs = keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "\n",
    "        model = keras.Model( \n",
    "            inputs = inputs, \n",
    "            outputs = outputs,\n",
    "            name = \"LSTMDense_model\"\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            loss=huber_loss,\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=0.0001,  # def lr = 0.001\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999, \n",
    "                epsilon=1e-07\n",
    "            ),\n",
    "            metrics=keras.metrics.MSE,\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 21\n",
    "\n",
    "input_dim = size_x\n",
    "output_size = Ny * size_y\n",
    "\n",
    "units = max(input_dim, output_size)\n",
    "\n",
    "print(input_dim, units, output_size)\n",
    "\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "        lstm_layer = RegularizedLSTM(units, input_shape=(None, input_dim), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer1 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer2 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer3 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer4 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer5 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer6 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer7 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer8 = RegularizedLSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "        lstm_layer9 = keras.layers.LSTM(units, input_shape=(None, units), kernel_initializer=\"lecun_normal\" )\n",
    "\n",
    "    else:\n",
    "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "        lstm_layer = keras.layers.RNN(keras.layers.LSTMCell(units), input_shape=(None, input_dim), return_sequences=True)\n",
    "\n",
    "    # with mirrored_strategy.scope():\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            lstm_layer1,\n",
    "            lstm_layer2,\n",
    "            lstm_layer3,\n",
    "            lstm_layer4,\n",
    "            lstm_layer5,\n",
    "            lstm_layer6,\n",
    "            lstm_layer7,\n",
    "            lstm_layer8,\n",
    "            lstm_layer9,\n",
    "            keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "            keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "            keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "            keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "            keras.layers.Dense(output_size, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=huber_loss,\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=0.001,  # def lr = 0.001\n",
    "            beta_1=0.9, \n",
    "            beta_2=0.999, \n",
    "            epsilon=1e-07\n",
    "        ),\n",
    "        metrics=keras.metrics.MSE,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Build model ====================\n",
    "\n",
    "# model = build_model(size_x, Ny * size_y, allow_cudnn_kernel=True)\n",
    "model = build_model(allow_cudnn_kernel=True)\n",
    "model.summary()\n",
    "# keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Fit model ====================\n",
    "\n",
    "# options = tf.data.Options()\n",
    "# options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "# train_data = Dataset_train.with_options(options)\n",
    "# val_data = Dataset_valid.with_options(options)\n",
    "\n",
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=5)\n",
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Define plot_history ====================\n",
    "\n",
    "def plot_history(history, loss=\"loss\"):\n",
    "    train_losses = history.history[loss]\n",
    "    valid_losses = history.history[\"val_\" + loss]\n",
    "    n_epochs = len(history.epoch)\n",
    "    minloss = min( np.min(valid_losses), np.min(train_losses) )\n",
    "    maxloss = max( np.max(valid_losses), np.max(train_losses) )\n",
    "    \n",
    "    plt.plot(train_losses, color=\"b\", label=\"Train\")\n",
    "    plt.plot(valid_losses, color=\"r\", label=\"Validation\")\n",
    "    plt.plot([0, n_epochs], [minloss, minloss], \"k--\",\n",
    "             label=\"Min val: {:.2f}\".format(minloss))\n",
    "    plt.axis([0, n_epochs, minloss/1.05, maxloss*1.05])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_2= model.fit(Dataset_train, validation_data=Dataset_valid, epochs=10, initial epoch=history_1.epoch(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=5)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=5)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Dataset_train, validation_data=Dataset_valid, epochs=5)\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
