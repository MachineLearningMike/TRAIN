{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print( len(gpus) )\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
    "text_file = tf.keras.utils.get_file(\n",
    "    fname=\"fra-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "# show where the file is located now\n",
    "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
    "print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
    "text_file = tf.keras.utils.get_file(\n",
    "    fname=\"fra-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
    "\n",
    "def normalize(line):\n",
    "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
    "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
    "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
    "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
    "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
    "    eng, fra = line.split(\"\\t\")\n",
    "    fra = \"[start] \" + fra + \" [end]\"\n",
    "    return eng, fra\n",
    "\n",
    "# normalize each line and separate into English and French\n",
    "with open(text_file) as fp:\n",
    "    text_pairs = [normalize(line) for line in fp]\n",
    "\n",
    "# print some samples\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "\n",
    "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# count tokens\n",
    "eng_tokens, fra_tokens = set(), set()\n",
    "eng_maxlen, fra_maxlen = 0, 0\n",
    "for eng, fra in text_pairs:\n",
    "    eng_tok, fra_tok = eng.split(), fra.split()\n",
    "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
    "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
    "    eng_tokens.update(eng_tok)\n",
    "    fra_tokens.update(fra_tok)\n",
    "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
    "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
    "print(f\"Max English length: {eng_maxlen}\")\n",
    "print(f\"Max French length: {fra_maxlen}\")\n",
    "print(f\"{len(text_pairs)} total pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# histogram of sentence length in tokens\n",
    "en_lengths = [len(eng.split()) for eng, fra in text_pairs]\n",
    "fr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n",
    "\n",
    "plt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\n",
    "plt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\n",
    "plt.yscale(\"log\")     # sentence length fits Benford\"s law\n",
    "plt.ylim(plt.ylim())  # make y-axis consistent for both plots\n",
    "plt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\n",
    "plt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\n",
    "plt.legend()\n",
    "plt.title(\"Examples count vs Token length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load normalized sentence pairs\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# train-test-val split of randomized sentence pairs\n",
    "random.shuffle(text_pairs)\n",
    "n_val = int(0.15*len(text_pairs))\n",
    "n_train = len(text_pairs) - 2*n_val\n",
    "train_pairs = text_pairs[:n_train]\n",
    "val_pairs = text_pairs[n_train:n_train+n_val]\n",
    "test_pairs = text_pairs[n_train+n_val:]\n",
    "\n",
    "# Parameter determined after analyzing the input data\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "seq_length = 20\n",
    "\n",
    "# Create vectorizer\n",
    "eng_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_en,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length,\n",
    ")\n",
    "fra_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_fr,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length + 1\n",
    ")\n",
    "\n",
    "# train the vectorization layer using training dataset\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_fra_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorizer.adapt(train_eng_texts)\n",
    "fra_vectorizer.adapt(train_fra_texts)\n",
    "\n",
    "# save for subsequent steps\n",
    "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
    "    data = {\n",
    "        \"train\": train_pairs,\n",
    "        \"val\":   val_pairs,\n",
    "        \"test\":  test_pairs,\n",
    "        \"engvec_config\":  eng_vectorizer.get_config(),\n",
    "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
    "        \"fravec_config\":  fra_vectorizer.get_config(),\n",
    "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
    "    }\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# load text data and vectorizer weights\n",
    "with open(\"vectorize.pickle\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "train_pairs = data[\"train\"]\n",
    "val_pairs = data[\"val\"]\n",
    "test_pairs = data[\"test\"]   # not used\n",
    "\n",
    "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
    "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
    "fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n",
    "fra_vectorizer.set_weights(data[\"fravec_weights\"])\n",
    "\n",
    "# set up Dataset object\n",
    "def format_dataset(eng, fra):\n",
    "    \"\"\"Take an English and a French sentence pair, convert into input and target.\n",
    "    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n",
    "    is a vector, corresponding to English and French sentences respectively.\n",
    "    The target is also vector of the French sentence, advanced by 1 token. All\n",
    "    vector are in the same length.\n",
    "\n",
    "    The output will be used for training the transformer model. In the model we\n",
    "    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n",
    "    which should be matched to the keys in the dictionary for the source part\n",
    "    \"\"\"\n",
    "    eng = eng_vectorizer(eng)\n",
    "    fra = fra_vectorizer(fra)\n",
    "    source = {\"encoder_inputs\": eng,\n",
    "              \"decoder_inputs\": fra[:, :-1]}\n",
    "    target = fra[:, 1:]\n",
    "    return (source, target)\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n",
    "    # aggregate sentences using zip(*pairs)\n",
    "    eng_texts, fra_texts = zip(*pairs)\n",
    "    # convert them into list, and then create tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n",
    "    return dataset.shuffle(2048) \\\n",
    "                  .batch(batch_size).map(format_dataset) \\\n",
    "                  .prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# test the dataset\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "    print(f\"targets[0]: {targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pos_enc_matrix(L, d, n=10000):\n",
    "    \"\"\"Create positional encoding matrix\n",
    "\n",
    "    Args:\n",
    "        L: Input dimension (length)\n",
    "        d: Output dimension (depth), even only\n",
    "        n: Constant for the sinusoidal functions\n",
    "\n",
    "    Returns:\n",
    "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
    "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
    "    \"\"\"\n",
    "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "    d2 = d//2\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
    "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
    "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
    "    args = k * denom                    # (L,d) matrix\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P\n",
    "\n",
    "\n",
    "# Plot the positional encoding matrix\n",
    "pos_matrix = pos_enc_matrix(L=2048, d=512)\n",
    "assert pos_matrix.shape == (2048, 512)\n",
    "plt.pcolormesh(pos_matrix, cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "with open(\"posenc-2048-512.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(pos_matrix, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
    "    pos_matrix = pickle.load(fp)\n",
    "assert pos_matrix.shape == (2048, 512)\n",
    "# Plot the positional encoding matrix, alternative way\n",
    "plt.pcolormesh(np.hstack([pos_matrix[:, ::2], pos_matrix[:, 1::2]]), cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "plt.plot(pos_matrix[:, 155], label=\"high freq\")\n",
    "plt.plot(pos_matrix[:, 300], label=\"low freq\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
    "    pos_matrix = pickle.load(fp)\n",
    "assert pos_matrix.shape == (2048, 512)\n",
    "# Plot two curves from different position\n",
    "plt.plot(pos_matrix[100], alpha=0.66, color=\"red\", label=\"position 100\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
    "    pos_matrix = pickle.load(fp)\n",
    "assert pos_matrix.shape == (2048, 512)\n",
    "# Show the dot product between different normalized positional vectors\n",
    "pos_matrix /= np.linalg.norm(pos_matrix, axis=1, keepdims=True)\n",
    "p = pos_matrix[789]  # all vectors compare to vector at position 789\n",
    "dots = pos_matrix @ p\n",
    "plt.plot(dots)\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def pos_enc_matrix(L, d, n=10000):\n",
    "    \"\"\"Create positional encoding matrix\n",
    "\n",
    "    Args:\n",
    "        L: Input dimension (length)\n",
    "        d: Output dimension (depth), even only\n",
    "        n: Constant for the sinusoidal functions\n",
    "\n",
    "    Returns:\n",
    "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
    "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
    "    \"\"\"\n",
    "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "    d2 = d//2\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
    "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
    "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
    "    args = k * denom                    # (L,d) matrix\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
    "    embedding and returns positional-encoded output.\"\"\"\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_length: Input sequence length\n",
    "            vocab_size: Input vocab size, for setting up embedding matrix\n",
    "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim     # d_model in paper\n",
    "        # token embedding layer: Convert integer token to D-dim float vector\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n",
    "        )\n",
    "        # positional embedding layer: a matrix of hard-coded sine values\n",
    "        matrix = pos_enc_matrix(sequence_length, embed_dim)\n",
    "        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
    "        with position vectors\"\"\"\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + self.position_embeddings\n",
    "\n",
    "    # this layer is using an Embedding layer, which can take a mask\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Lesson 03:\n",
    "# train_ds = make_dataset(train_pairs)\n",
    "\n",
    "vocab_size_en = 10000\n",
    "seq_length = 20\n",
    "\n",
    "# test the dataset\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs[\"encoder_inputs\"])\n",
    "    embed_en = PositionalEmbedding(seq_length, vocab_size_en, embed_dim=512)\n",
    "    en_emb = embed_en(inputs[\"encoder_inputs\"])\n",
    "    print(en_emb.shape)\n",
    "    print(en_emb._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def self_attention(input_shape, prefix=\"att\", **kwargs):\n",
    "    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n",
    "    input is the output from positional encoding layer.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): The prefix added to the layer names\n",
    "        masked (bool): whether to use causal mask. Should be False on encoder and\n",
    "                       True on decoder. When True, a mask will be applied such that\n",
    "                       each location only has access to the locations before it.\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in1\")\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n",
    "    # functional API to connect input to output\n",
    "    attout = attention(query=inputs, value=inputs, key=inputs)\n",
    "    outputs = norm(add([inputs, attout]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n",
    "    return model\n",
    "\n",
    "seq_length = 20\n",
    "key_dim = 128\n",
    "num_heads = 8\n",
    "\n",
    "model = self_attention(input_shape=(seq_length, key_dim),\n",
    "                       num_heads=num_heads, key_dim=key_dim)\n",
    "tf.keras.utils.plot_model(model, \"self-attention.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n",
    "    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n",
    "    input is the output from positional encoding layer at decoder\n",
    "    and context is the final output from encoder.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): The prefix added to the layer names\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n",
    "                                    name=f\"{prefix}_ctx2\")\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in2\")\n",
    "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n",
    "    # functional API to connect input to output\n",
    "    attout = attention(query=inputs, value=context, key=context)\n",
    "    outputs = norm(add([attout, inputs]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n",
    "                           name=f\"{prefix}_cross\")\n",
    "    return model\n",
    "\n",
    "seq_length = 20\n",
    "key_dim = 128\n",
    "num_heads = 8\n",
    "\n",
    "model = cross_attention(input_shape=(seq_length, key_dim),\n",
    "                        context_shape=(seq_length, key_dim),\n",
    "                        num_heads=num_heads, key_dim=key_dim)\n",
    "tf.keras.utils.plot_model(model, \"cross-attention.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n",
    "    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n",
    "    input is the output from an attention layer with add & norm, the output\n",
    "    is the output of one encoder or decoder block\n",
    "\n",
    "    Args:\n",
    "        model_dim (int): Output dimension of the feed-forward layer, which\n",
    "                         is also the output dimension of the encoder/decoder\n",
    "                         block\n",
    "        ff_dim (int): Internal dimension of the feed-forward layer\n",
    "        dropout (float): Dropout rate\n",
    "        prefix (str): The prefix added to the layer names\n",
    "    \"\"\"\n",
    "    # create layers\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in3\")\n",
    "    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n",
    "    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n",
    "    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n",
    "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
    "    # functional API to connect input to output\n",
    "    ffout = drop(dense2(dense1(inputs)))\n",
    "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n",
    "    outputs = norm(add([inputs, ffout]))\n",
    "    # create model and return\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n",
    "    return model\n",
    "\n",
    "seq_length = 20\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "\n",
    "model = feed_forward(input_shape=(seq_length, key_dim),\n",
    "                     model_dim=key_dim, ff_dim=ff_dim)\n",
    "tf.keras.utils.plot_model(model, \"feedforward.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n",
    "    \"\"\"One encoder unit. The input and output are in the same shape so we can\n",
    "    daisy chain multiple encoder units into one larger encoder\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n",
    "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, **kwargs),\n",
    "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n",
    "    ], name=prefix)\n",
    "    return model\n",
    "\n",
    "\n",
    "seq_length = 20\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "model = encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
    "                num_heads=num_heads)\n",
    "tf.keras.utils.plot_model(model, \"encoder.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n",
    "    \"\"\"One decoder unit. The input and output are in the same shape so we can\n",
    "    daisy chain multiple decoder units into one larger decoder. The context\n",
    "    vector is also assumed to be the same shape for convenience\"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                   name=f\"{prefix}_in0\")\n",
    "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
    "                                    name=f\"{prefix}_ctx0\")\n",
    "    attmodel = self_attention(input_shape, key_dim=key_dim, prefix=prefix, **kwargs)\n",
    "    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n",
    "                                 prefix=prefix, **kwargs)\n",
    "    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
    "    x = attmodel(inputs)\n",
    "    x = crossmodel([(context, x)])\n",
    "    output = ffmodel(x)\n",
    "    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n",
    "    return model\n",
    "\n",
    "\n",
    "seq_length = 20\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "model = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
    "                num_heads=num_heads)\n",
    "tf.keras.utils.plot_model(model, \"decoder.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n",
    "                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n",
    "    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n",
    "    # set up layers\n",
    "    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
    "                                      name=\"encoder_inputs\")\n",
    "    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
    "                                      name=\"decoder_inputs\")\n",
    "    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n",
    "    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n",
    "    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n",
    "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n",
    "                        num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n",
    "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n",
    "                        num_heads=num_heads)\n",
    "                for i in range(num_layers)]\n",
    "    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n",
    "    # build output\n",
    "    x1 = embed_enc(input_enc)\n",
    "    x2 = embed_dec(input_dec)\n",
    "    for layer in encoders:\n",
    "        x1 = layer(x1)\n",
    "    for layer in decoders:\n",
    "        x2 = layer([x2, x1])\n",
    "    output = final(x2)\n",
    "    # XXX keep this try-except block\n",
    "    try:\n",
    "        del output._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "seq_len = 20\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "dropout = 0.1\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
    "                    vocab_size_en, vocab_size_fr, dropout)\n",
    "tf.keras.utils.plot_model(model, \"transformer.png\",\n",
    "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
    "                          rankdir='BT', show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"Custom learning rate for Adam optimizer\"\n",
    "    def __init__(self, key_dim, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.key_dim = key_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.d = tf.cast(self.key_dim, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible0\n",
    "        config = {\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "key_dim = 128\n",
    "lr = CustomSchedule(key_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "plt.plot(lr(tf.range(50000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "seq_len = 20\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "dropout = 0.1\n",
    "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
    "                    vocab_size_en, vocab_size_fr, dropout)\n",
    "lr = CustomSchedule(key_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create and train the model\n",
    "seq_len = 20\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "key_dim = 128\n",
    "ff_dim = 512\n",
    "dropout = 0.1\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
    "                    vocab_size_en, vocab_size_fr, dropout)\n",
    "lr = CustomSchedule(key_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
    "epochs = 20\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"eng-fra-transformer.h5\")\n",
    "\n",
    "# Plot the loss and accuracy history\n",
    "fig, axs = plt.subplots(2, figsize=(6, 8), sharex=True)\n",
    "fig.suptitle('Traininig history')\n",
    "x = list(range(1, epochs+1))\n",
    "axs[0].plot(x, history.history[\"loss\"], alpha=0.5, label=\"loss\")\n",
    "axs[0].plot(x, history.history[\"val_loss\"], alpha=0.5, label=\"val_loss\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[1].plot(x, history.history[\"masked_accuracy\"], alpha=0.5, label=\"acc\")\n",
    "axs[1].plot(x, history.history[\"val_masked_accuracy\"], alpha=0.5, label=\"val_acc\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "custom_objects = {\"PositionalEmbedding\": PositionalEmbedding,\n",
    "                  \"CustomSchedule\": CustomSchedule,\n",
    "                  \"masked_loss\": masked_loss,\n",
    "                  \"masked_accuracy\": masked_accuracy}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model = tf.keras.models.load_model(\"eng-fra-transformer.h5\")\n",
    "\n",
    "# training parameters used\n",
    "seq_len = 20\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"Create the translated sentence\"\"\"\n",
    "    enc_tokens = eng_vectorizer([sentence])\n",
    "    lookup = list(fra_vectorizer.get_vocabulary())\n",
    "    start_sentinel, end_sentinel = \"[start]\", \"[end]\"\n",
    "    output_sentence = [start_sentinel]\n",
    "    # generate the translated sentence word by word\n",
    "    for i in range(seq_len):\n",
    "        vector = fra_vectorizer([\" \".join(output_sentence)])\n",
    "        assert vector.shape == (1, seq_len+1)\n",
    "        dec_tokens = vector[:, :-1]\n",
    "        assert dec_tokens.shape == (1, seq_len)\n",
    "        pred = model([enc_tokens, dec_tokens])\n",
    "        assert pred.shape == (1, seq_len, vocab_size_fr)\n",
    "        word = lookup[np.argmax(pred[0, i, :])]\n",
    "        output_sentence.append(word)\n",
    "        if word == end_sentinel:\n",
    "            break\n",
    "    return output_sentence\n",
    "\n",
    "test_count = 20\n",
    "for n in range(test_count):\n",
    "    english_sentence, french_sentence = random.choice(test_pairs)\n",
    "    translated = translate(english_sentence)\n",
    "    print(f\"Test {n}:\")\n",
    "    print(f\"{english_sentence}\")\n",
    "    print(f\"== {french_sentence}\")\n",
    "    print(f\"-> {' '.join(translated)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You did it!\n",
    "\n",
    "Let’s go back and review what you did and what can be improved. You made a transformer model that takes an entire English sentence and a partial French sentence (up to the \n",
    "-th token) to predict the next (the \n",
    "-th) token.\n",
    "\n",
    "In training, you observed that the accuracy is at 70% to 80% at the best. How can you improve it? Here are some ideas, but surely, not exhaustive:\n",
    "\n",
    "- You used a simple tokenizer for your text input. Libraries such as NLTK can provide better tokenizers. Also, you didn’t use subword tokenization. It is less a problem for English but problematic for French. That’s why you have vastly larger vocabulary size in French in your model (e.g., l’air (the air) and d’air (of air) would become distinct tokens).\n",
    "- You trained your own word embeddings with an embedding layer. There are pre-trained embeddings (such as GloVe) readily available, and they usually provide better quality embeddings. This may help your model to understand the context better.\n",
    "- You designed the transformer with some parameters. You used 8 heads for multi-head attention, output vector dimension is 128, sentence length was limited to 20 tokens, drop out rate is 0.1, and so on. Tuning these parameters will surely impact the transformer one way or another. Similarly important are the training parameters such as number of epochs, learning rate schedule, and loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You made it. Well done!\n",
    "\n",
    "Take a moment and look back at how far you have come.\n",
    "\n",
    "- You learned how to take a plaintext sentence, process it, and vectorize it\n",
    "- You analyzed the building block of a transformer model according to the paper Attention Is All You Need, and implemented each building block using Keras\n",
    "- You connected the building blocks into a complete transformer model, and train it\n",
    "- Finally, you can witness the trained model to translate English sentences into French with high accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
